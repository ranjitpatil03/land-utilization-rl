[23:58:38] Test log message
[23:58:38] RL Training Summary: {'test': 'metric', 'value': 1.5}
[23:58:44] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[23:58:44] InputAgent: Loading case from data\inputs\mumbai_case.json
[23:58:44] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[23:58:44] Would download: https://example.com/regulations.pdf
[23:58:44] Parsing (stub): rules_kb/sample_rules.json
[23:58:44] Classification Agent: Starting to select applicable rules.
[23:58:44] Classification Agent: SELECTED rule R-SETBACK-URBAN
[23:58:44] Classification Agent: SELECTED rule R-COVERAGE
[23:58:44] Classification Agent: SELECTED rule R-FAR
[23:58:44] Classification Agent: Found 3 applicable rules.
[23:58:44] Classification Agent: Decision details - SELECTED rule R-SETBACK-URBAN: Condition match: location=urban, SELECTED rule R-COVERAGE: No conditions, SELECTED rule R-FAR: No conditions
[23:58:44] Calculation Agent: Starting computations.
[23:58:44] Calculation Agent: Processing case with plot size: 500 sqm
[23:58:44] Calculation Agent: Location is urban
[23:58:44] Calculation Agent: Evaluating setback for road width 9
[23:58:44] Calculation Agent: Using default setback value: 1.5
[23:58:44] Calculation Agent: Coverage for urban is 0.6
[23:58:44] Calculation Agent: FAR for urban is 1.8
[23:58:44] Calculation Agent: Max footprint: 300.0 sqm, Total floor area: 900.0 sqm
[23:58:44] Calculation Agent: Finished. Total floor area: 900.0 sqm.
[23:58:44] RL Agent: Starting RL decision process.
[23:58:44] RL Agent: Expected rule path: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Agent: Candidate 1 (expected): ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Agent: Candidate 2 (reversed): ['R-FAR', 'R-SETBACK-URBAN', 'R-COVERAGE']
[23:58:44] RL Agent: Candidate 3 (rotated): ['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE']
[23:58:44] RL Agent: Initializing RL environment and training...
[23:58:44] RL Environment: Initialized with 3 candidate paths
[23:58:44] RL Environment: Candidate 0: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Environment: Candidate 1: ['R-FAR', 'R-SETBACK-URBAN', 'R-COVERAGE']
[23:58:44] RL Environment: Candidate 2: ['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE']
[23:58:44] Training RL (random policy) for 10 episodes...
[23:58:44] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[23:58:44] RL Environment: Reset
[23:58:44] RL Environment: Action 0 -> Chosen: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Expected: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Reward: 1
[23:58:44] RL Training - Episode 1: {'episode': 1, 'action': 0, 'reward': 1, 'chosen_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'expected_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:58:44] EP 1: action=0, reward=1, chosen=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'] expected=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Environment: Reset
[23:58:44] RL Environment: Action 0 -> Chosen: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Expected: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Reward: 1
[23:58:44] RL Training - Episode 2: {'episode': 2, 'action': 0, 'reward': 1, 'chosen_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'expected_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:58:44] EP 2: action=0, reward=1, chosen=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'] expected=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Environment: Reset
[23:58:44] RL Environment: Action 2 -> Chosen: ['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Reward: -1
[23:58:44] RL Training - Episode 3: {'episode': 3, 'action': 2, 'reward': -1, 'chosen_path': ['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'running_avg_reward': 0.3333333333333333, 'success_rate': 0.6666666666666666}
[23:58:44] EP 3: action=2, reward=-1, chosen=['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Environment: Reset
[23:58:44] RL Environment: Action 0 -> Chosen: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Expected: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Reward: 1
[23:58:44] RL Training - Episode 4: {'episode': 4, 'action': 0, 'reward': 1, 'chosen_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'expected_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'running_avg_reward': 0.5, 'success_rate': 0.75}
[23:58:44] EP 4: action=0, reward=1, chosen=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'] expected=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Environment: Reset
[23:58:44] RL Environment: Action 0 -> Chosen: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Expected: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Reward: 1
[23:58:44] RL Training - Episode 5: {'episode': 5, 'action': 0, 'reward': 1, 'chosen_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'expected_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'running_avg_reward': 0.6, 'success_rate': 0.8}
[23:58:44] EP 5: action=0, reward=1, chosen=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'] expected=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Environment: Reset
[23:58:44] RL Environment: Action 1 -> Chosen: ['R-FAR', 'R-SETBACK-URBAN', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Reward: -1
[23:58:44] RL Training - Episode 6: {'episode': 6, 'action': 1, 'reward': -1, 'chosen_path': ['R-FAR', 'R-SETBACK-URBAN', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'running_avg_reward': 0.3333333333333333, 'success_rate': 0.6666666666666666}
[23:58:44] EP 6: action=1, reward=-1, chosen=['R-FAR', 'R-SETBACK-URBAN', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Environment: Reset
[23:58:44] RL Environment: Action 2 -> Chosen: ['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Reward: -1
[23:58:44] RL Training - Episode 7: {'episode': 7, 'action': 2, 'reward': -1, 'chosen_path': ['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'running_avg_reward': 0.14285714285714285, 'success_rate': 0.5714285714285714}
[23:58:44] EP 7: action=2, reward=-1, chosen=['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Environment: Reset
[23:58:44] RL Environment: Action 2 -> Chosen: ['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Reward: -1
[23:58:44] RL Training - Episode 8: {'episode': 8, 'action': 2, 'reward': -1, 'chosen_path': ['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[23:58:44] EP 8: action=2, reward=-1, chosen=['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Environment: Reset
[23:58:44] RL Environment: Action 0 -> Chosen: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Expected: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Reward: 1
[23:58:44] RL Training - Episode 9: {'episode': 9, 'action': 0, 'reward': 1, 'chosen_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'expected_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'running_avg_reward': 0.1111111111111111, 'success_rate': 0.5555555555555556}
[23:58:44] EP 9: action=0, reward=1, chosen=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'] expected=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] RL Environment: Reset
[23:58:44] RL Environment: Action 2 -> Chosen: ['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], Reward: -1
[23:58:44] RL Training - Episode 10: {'episode': 10, 'action': 2, 'reward': -1, 'chosen_path': ['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[23:58:44] EP 10: action=2, reward=-1, chosen=['R-SETBACK-URBAN', 'R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-SETBACK-URBAN', 'R-FAR']
[23:58:44] Avg reward: 0.00, Success rate: 50.00%
[23:58:44] RL Training Summary: {'status': 'training_completed', 'avg_reward': 0.0, 'success_rate': 0.5, 'episodes': 10, 'total_reward': 0}
[23:58:44] RL Agent: Training complete. Metrics: {'avg_reward': 0.0, 'success_rate': 0.5, 'episodes': 10}
[23:58:44] RL Agent: Process complete.
[23:58:44] Orchestrator: Saved JSON report to io/outputs\json\mumbai_output.json
[23:58:44] Orchestrator: Saved geometry to io/outputs\geometry\mumbai_model.stl
[23:58:44] Orchestrator: Run complete.
[23:58:51] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[23:58:51] InputAgent: Loading case from data\inputs\mumbai_case.json
[23:58:51] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[23:58:51] Would download: https://example.com/regulations.pdf
[23:58:51] Parsing (stub): rules_kb/sample_rules.json
[23:58:51] Classification Agent: Starting to select applicable rules.
[23:58:51] Classification Agent: SELECTED rule R-SETBACK-URBAN
[23:58:51] Classification Agent: SELECTED rule R-COVERAGE
[23:58:51] Classification Agent: SELECTED rule R-FAR
[23:58:51] Classification Agent: Found 3 applicable rules.
[23:58:51] Classification Agent: Decision details - SELECTED rule R-SETBACK-URBAN: Condition match: location=urban, SELECTED rule R-COVERAGE: No conditions, SELECTED rule R-FAR: No conditions
[23:58:51] Calculation Agent: Starting computations.
[23:58:51] Calculation Agent: Processing case with plot size: 500 sqm
[23:58:51] Calculation Agent: Location is urban
[23:58:51] Calculation Agent: Evaluating setback for road width 9
[23:58:51] Calculation Agent: Using default setback value: 1.5
[23:58:51] Calculation Agent: Coverage for urban is 0.6
[23:58:51] Calculation Agent: FAR for urban is 1.8
[23:58:51] Calculation Agent: Max footprint: 300.0 sqm, Total floor area: 900.0 sqm
[23:58:51] Calculation Agent: Finished. Total floor area: 900.0 sqm.
[23:58:51] RL Agent: Starting RL decision process.
[23:58:51] RL Agent: Expected rule path: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Agent: Candidate 1 (expected): ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Agent: Candidate 2 (reversed): ['R-FAR', 'R-COVERAGE', 'R-SETBACK-URBAN']
[23:58:51] RL Agent: Candidate 3 (rotated): ['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN']
[23:58:51] RL Agent: Initializing RL environment and training...
[23:58:51] RL Environment: Initialized with 3 candidate paths
[23:58:51] RL Environment: Candidate 0: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Candidate 1: ['R-FAR', 'R-COVERAGE', 'R-SETBACK-URBAN']
[23:58:51] RL Environment: Candidate 2: ['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN']
[23:58:51] Training RL (random policy) for 10 episodes...
[23:58:51] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 0 -> Chosen: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Expected: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Reward: 1
[23:58:51] RL Training - Episode 1: {'episode': 1, 'action': 0, 'reward': 1, 'chosen_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'expected_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:58:51] EP 1: action=0, reward=1, chosen=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'] expected=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 0 -> Chosen: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Expected: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Reward: 1
[23:58:51] RL Training - Episode 2: {'episode': 2, 'action': 0, 'reward': 1, 'chosen_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'expected_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:58:51] EP 2: action=0, reward=1, chosen=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'] expected=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 0 -> Chosen: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Expected: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Reward: 1
[23:58:51] RL Training - Episode 3: {'episode': 3, 'action': 0, 'reward': 1, 'chosen_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'expected_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:58:51] EP 3: action=0, reward=1, chosen=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'] expected=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 0 -> Chosen: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Expected: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Reward: 1
[23:58:51] RL Training - Episode 4: {'episode': 4, 'action': 0, 'reward': 1, 'chosen_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'expected_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:58:51] EP 4: action=0, reward=1, chosen=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'] expected=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 0 -> Chosen: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Expected: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Reward: 1
[23:58:51] RL Training - Episode 5: {'episode': 5, 'action': 0, 'reward': 1, 'chosen_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'expected_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:58:51] EP 5: action=0, reward=1, chosen=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'] expected=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 2 -> Chosen: ['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN'], Expected: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 6: {'episode': 6, 'action': 2, 'reward': -1, 'chosen_path': ['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN'], 'expected_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'running_avg_reward': 0.6666666666666666, 'success_rate': 0.8333333333333334}
[23:58:51] EP 6: action=2, reward=-1, chosen=['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN'] expected=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 1 -> Chosen: ['R-FAR', 'R-COVERAGE', 'R-SETBACK-URBAN'], Expected: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 7: {'episode': 7, 'action': 1, 'reward': -1, 'chosen_path': ['R-FAR', 'R-COVERAGE', 'R-SETBACK-URBAN'], 'expected_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'running_avg_reward': 0.42857142857142855, 'success_rate': 0.7142857142857143}
[23:58:51] EP 7: action=1, reward=-1, chosen=['R-FAR', 'R-COVERAGE', 'R-SETBACK-URBAN'] expected=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 0 -> Chosen: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Expected: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Reward: 1
[23:58:51] RL Training - Episode 8: {'episode': 8, 'action': 0, 'reward': 1, 'chosen_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'expected_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'running_avg_reward': 0.5, 'success_rate': 0.75}
[23:58:51] EP 8: action=0, reward=1, chosen=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'] expected=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 2 -> Chosen: ['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN'], Expected: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 9: {'episode': 9, 'action': 2, 'reward': -1, 'chosen_path': ['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN'], 'expected_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'running_avg_reward': 0.3333333333333333, 'success_rate': 0.6666666666666666}
[23:58:51] EP 9: action=2, reward=-1, chosen=['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN'] expected=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 2 -> Chosen: ['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN'], Expected: ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 10: {'episode': 10, 'action': 2, 'reward': -1, 'chosen_path': ['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN'], 'expected_path': ['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR'], 'running_avg_reward': 0.2, 'success_rate': 0.6}
[23:58:51] EP 10: action=2, reward=-1, chosen=['R-COVERAGE', 'R-FAR', 'R-SETBACK-URBAN'] expected=['R-SETBACK-URBAN', 'R-COVERAGE', 'R-FAR']
[23:58:51] Avg reward: 0.20, Success rate: 60.00%
[23:58:51] RL Training Summary: {'status': 'training_completed', 'avg_reward': 0.2, 'success_rate': 0.6, 'episodes': 10, 'total_reward': 2}
[23:58:51] RL Agent: Training complete. Metrics: {'avg_reward': 0.2, 'success_rate': 0.6, 'episodes': 10}
[23:58:51] RL Agent: Process complete.
[23:58:51] Orchestrator: Saved JSON report to io/outputs\json\mumbai_output.json
[23:58:51] Orchestrator: Saved geometry to io/outputs\geometry\mumbai_model.stl
[23:58:51] Orchestrator: Run complete.
[23:58:51] Orchestrator: Starting run for case: data/inputs/pune_case.json
[23:58:51] InputAgent: Loading case from data\inputs\pune_case.json
[23:58:51] InputAgent: Successfully loaded case with keys: ['city', 'type', 'plot_size']
[23:58:51] Would download: https://example.com/regulations.pdf
[23:58:51] Parsing (stub): rules_kb/sample_rules.json
[23:58:51] Classification Agent: Starting to select applicable rules.
[23:58:51] Classification Agent: REJECTED rule R-SETBACK-URBAN
[23:58:51] Classification Agent: SELECTED rule R-COVERAGE
[23:58:51] Classification Agent: SELECTED rule R-FAR
[23:58:51] Classification Agent: Found 2 applicable rules.
[23:58:51] Classification Agent: Decision details - REJECTED rule R-SETBACK-URBAN: Condition mismatch: location=None, expected urban, SELECTED rule R-COVERAGE: No conditions, SELECTED rule R-FAR: No conditions
[23:58:51] Calculation Agent: Starting computations.
[23:58:51] Calculation Agent: Processing case with plot size: 400 sqm
[23:58:51] Calculation Agent: Location is default
[23:58:51] Calculation Agent: No specific setback rule found, using default: 1.5
[23:58:51] Calculation Agent: Coverage for default is 0.55
[23:58:51] Calculation Agent: FAR for default is 1.5
[23:58:51] Calculation Agent: Max footprint: 220.00000000000003 sqm, Total floor area: 600.0 sqm
[23:58:51] Calculation Agent: Finished. Total floor area: 600.0 sqm.
[23:58:51] RL Agent: Starting RL decision process.
[23:58:51] RL Agent: Expected rule path: ['R-COVERAGE', 'R-FAR']
[23:58:51] RL Agent: Candidate 1 (expected): ['R-COVERAGE', 'R-FAR']
[23:58:51] RL Agent: Candidate 2 (reversed): ['R-FAR', 'R-COVERAGE']
[23:58:51] RL Agent: Candidate 3 (rotated): ['R-FAR', 'R-COVERAGE']
[23:58:51] RL Agent: Initializing RL environment and training...
[23:58:51] RL Environment: Initialized with 3 candidate paths
[23:58:51] RL Environment: Candidate 0: ['R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Candidate 1: ['R-FAR', 'R-COVERAGE']
[23:58:51] RL Environment: Candidate 2: ['R-FAR', 'R-COVERAGE']
[23:58:51] Training RL (random policy) for 10 episodes...
[23:58:51] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 1 -> Chosen: ['R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 1: {'episode': 1, 'action': 1, 'reward': -1, 'chosen_path': ['R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-FAR'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[23:58:51] EP 1: action=1, reward=-1, chosen=['R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 1 -> Chosen: ['R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 2: {'episode': 2, 'action': 1, 'reward': -1, 'chosen_path': ['R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-FAR'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[23:58:51] EP 2: action=1, reward=-1, chosen=['R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 2 -> Chosen: ['R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 3: {'episode': 3, 'action': 2, 'reward': -1, 'chosen_path': ['R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-FAR'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[23:58:51] EP 3: action=2, reward=-1, chosen=['R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 0 -> Chosen: ['R-COVERAGE', 'R-FAR'], Expected: ['R-COVERAGE', 'R-FAR'], Reward: 1
[23:58:51] RL Training - Episode 4: {'episode': 4, 'action': 0, 'reward': 1, 'chosen_path': ['R-COVERAGE', 'R-FAR'], 'expected_path': ['R-COVERAGE', 'R-FAR'], 'running_avg_reward': -0.5, 'success_rate': 0.25}
[23:58:51] EP 4: action=0, reward=1, chosen=['R-COVERAGE', 'R-FAR'] expected=['R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 0 -> Chosen: ['R-COVERAGE', 'R-FAR'], Expected: ['R-COVERAGE', 'R-FAR'], Reward: 1
[23:58:51] RL Training - Episode 5: {'episode': 5, 'action': 0, 'reward': 1, 'chosen_path': ['R-COVERAGE', 'R-FAR'], 'expected_path': ['R-COVERAGE', 'R-FAR'], 'running_avg_reward': -0.2, 'success_rate': 0.4}
[23:58:51] EP 5: action=0, reward=1, chosen=['R-COVERAGE', 'R-FAR'] expected=['R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 1 -> Chosen: ['R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 6: {'episode': 6, 'action': 1, 'reward': -1, 'chosen_path': ['R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-FAR'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[23:58:51] EP 6: action=1, reward=-1, chosen=['R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 0 -> Chosen: ['R-COVERAGE', 'R-FAR'], Expected: ['R-COVERAGE', 'R-FAR'], Reward: 1
[23:58:51] RL Training - Episode 7: {'episode': 7, 'action': 0, 'reward': 1, 'chosen_path': ['R-COVERAGE', 'R-FAR'], 'expected_path': ['R-COVERAGE', 'R-FAR'], 'running_avg_reward': -0.14285714285714285, 'success_rate': 0.42857142857142855}
[23:58:51] EP 7: action=0, reward=1, chosen=['R-COVERAGE', 'R-FAR'] expected=['R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 1 -> Chosen: ['R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 8: {'episode': 8, 'action': 1, 'reward': -1, 'chosen_path': ['R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-FAR'], 'running_avg_reward': -0.25, 'success_rate': 0.375}
[23:58:51] EP 8: action=1, reward=-1, chosen=['R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 1 -> Chosen: ['R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 9: {'episode': 9, 'action': 1, 'reward': -1, 'chosen_path': ['R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-FAR'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[23:58:51] EP 9: action=1, reward=-1, chosen=['R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-FAR']
[23:58:51] RL Environment: Reset
[23:58:51] RL Environment: Action 2 -> Chosen: ['R-FAR', 'R-COVERAGE'], Expected: ['R-COVERAGE', 'R-FAR'], Reward: -1
[23:58:51] RL Training - Episode 10: {'episode': 10, 'action': 2, 'reward': -1, 'chosen_path': ['R-FAR', 'R-COVERAGE'], 'expected_path': ['R-COVERAGE', 'R-FAR'], 'running_avg_reward': -0.4, 'success_rate': 0.3}
[23:58:51] EP 10: action=2, reward=-1, chosen=['R-FAR', 'R-COVERAGE'] expected=['R-COVERAGE', 'R-FAR']
[23:58:51] Avg reward: -0.40, Success rate: 30.00%
[23:58:51] RL Training Summary: {'status': 'training_completed', 'avg_reward': -0.4, 'success_rate': 0.3, 'episodes': 10, 'total_reward': -4}
[23:58:51] RL Agent: Training complete. Metrics: {'avg_reward': -0.4, 'success_rate': 0.3, 'episodes': 10}
[23:58:51] RL Agent: Process complete.
[23:58:51] Orchestrator: Saved JSON report to io/outputs\json\pune_output.json
[23:58:51] Orchestrator: Saved geometry to io/outputs\geometry\pune_model.stl
[23:58:51] Orchestrator: Run complete.
[23:59:50] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[23:59:50] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[23:59:50] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[23:59:50] Would download: https://example.com/regulations.pdf
[23:59:50] Parsing (stub): rules_kb/sample_rules.json
[23:59:50] Classification Agent: Starting to select applicable rules.
[23:59:50] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[23:59:50] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[23:59:50] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[23:59:50] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[23:59:50] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[23:59:50] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[23:59:50] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[23:59:50] Classification Agent: REJECTED rule AHM-DCR-FAR
[23:59:50] Classification Agent: Found 5 applicable rules.
[23:59:50] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[23:59:50] Calculation Agent: Starting computations.
[23:59:50] Calculation Agent: Processing case with plot size: 800 sqm
[23:59:50] Calculation Agent: Location is urban
[23:59:50] Calculation Agent: No specific setback rule found, using default: 1.5
[23:59:50] Calculation Agent: Coverage for urban is 0.55
[23:59:50] Calculation Agent: FAR for urban is 1.5
[23:59:50] Calculation Agent: Max footprint: 440.00000000000006 sqm, Total floor area: 1200.0 sqm
[23:59:50] Calculation Agent: Finished. Total floor area: 1200.0 sqm.
[23:59:50] RL Agent: Starting RL decision process.
[23:59:50] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[23:59:50] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[23:59:50] RL Agent: Initializing RL environment and training...
[23:59:50] RL Environment: Initialized with 3 candidate paths
[23:59:50] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[23:59:50] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[23:59:50] Training RL (random policy) for 10 episodes...
[23:59:50] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[23:59:50] RL Environment: Reset
[23:59:50] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[23:59:50] RL Training - Episode 1: {'episode': 1, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[23:59:50] EP 1: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Environment: Reset
[23:59:50] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[23:59:50] RL Training - Episode 2: {'episode': 2, 'action': 2, 'reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[23:59:50] EP 2: action=2, reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Environment: Reset
[23:59:50] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: 1
[23:59:50] RL Training - Episode 3: {'episode': 3, 'action': 0, 'reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[23:59:50] EP 3: action=0, reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Environment: Reset
[23:59:50] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[23:59:50] RL Training - Episode 4: {'episode': 4, 'action': 2, 'reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.5, 'success_rate': 0.25}
[23:59:50] EP 4: action=2, reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Environment: Reset
[23:59:50] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[23:59:50] RL Training - Episode 5: {'episode': 5, 'action': 2, 'reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.6, 'success_rate': 0.2}
[23:59:50] EP 5: action=2, reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Environment: Reset
[23:59:50] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: 1
[23:59:50] RL Training - Episode 6: {'episode': 6, 'action': 0, 'reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[23:59:50] EP 6: action=0, reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Environment: Reset
[23:59:50] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: 1
[23:59:50] RL Training - Episode 7: {'episode': 7, 'action': 0, 'reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.14285714285714285, 'success_rate': 0.42857142857142855}
[23:59:50] EP 7: action=0, reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Environment: Reset
[23:59:50] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: 1
[23:59:50] RL Training - Episode 8: {'episode': 8, 'action': 0, 'reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[23:59:50] EP 8: action=0, reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Environment: Reset
[23:59:50] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[23:59:50] RL Training - Episode 9: {'episode': 9, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.1111111111111111, 'success_rate': 0.4444444444444444}
[23:59:50] EP 9: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] RL Environment: Reset
[23:59:50] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[23:59:50] RL Training - Episode 10: {'episode': 10, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.2, 'success_rate': 0.4}
[23:59:50] EP 10: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[23:59:50] Avg reward: -0.20, Success rate: 40.00%
[23:59:50] RL Training Summary: {'status': 'training_completed', 'avg_reward': -0.2, 'success_rate': 0.4, 'episodes': 10, 'total_reward': -2}
[23:59:50] RL Agent: Training complete. Metrics: {'avg_reward': -0.2, 'success_rate': 0.4, 'episodes': 10}
[23:59:50] RL Agent: Process complete.
[23:59:50] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[23:59:50] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[23:59:50] Orchestrator: Run complete.
[00:07:52] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[00:07:52] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[00:07:52] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[00:07:52] Would download: https://example.com/regulations.pdf
[00:07:52] Parsing (stub): rules_kb/sample_rules.json
[00:07:52] Classification Agent: Starting to select applicable rules.
[00:07:52] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:07:52] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[00:07:52] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[00:07:52] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[00:07:52] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[00:07:52] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:07:52] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:07:52] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:07:52] Classification Agent: Found 5 applicable rules.
[00:07:52] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[00:07:52] Calculation Agent: Starting computations.
[00:07:52] Calculation Agent: Processing case with plot size: 800 sqm
[00:07:52] Calculation Agent: Location is urban
[00:07:52] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[00:07:52] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[00:07:52] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[00:07:52] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:07:52] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[00:07:52] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[00:07:52] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[00:07:52] RL Agent: Starting RL decision process.
[00:07:52] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:07:52] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:07:52] RL Agent: Initializing RL environment and training...
[00:07:52] RL Environment: Initialized with 3 candidate paths
[00:07:52] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:07:52] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:07:52] Training RL (random policy) for 10 episodes...
[00:07:52] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:07:52] RL Environment: Reset
[00:07:52] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[00:07:52] RL Training - Episode 1: {'episode': 1, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:07:52] EP 1: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Environment: Reset
[00:07:52] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[00:07:52] RL Training - Episode 2: {'episode': 2, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:07:52] EP 2: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Environment: Reset
[00:07:52] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[00:07:52] RL Training - Episode 3: {'episode': 3, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:07:52] EP 3: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Environment: Reset
[00:07:52] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[00:07:52] RL Training - Episode 4: {'episode': 4, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:07:52] EP 4: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Environment: Reset
[00:07:52] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[00:07:52] RL Training - Episode 5: {'episode': 5, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:07:52] EP 5: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Environment: Reset
[00:07:52] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[00:07:52] RL Training - Episode 6: {'episode': 6, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:07:52] EP 6: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Environment: Reset
[00:07:52] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: 1
[00:07:52] RL Training - Episode 7: {'episode': 7, 'action': 0, 'reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.7142857142857143, 'success_rate': 0.14285714285714285}
[00:07:52] EP 7: action=0, reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Environment: Reset
[00:07:52] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[00:07:52] RL Training - Episode 8: {'episode': 8, 'action': 2, 'reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.75, 'success_rate': 0.125}
[00:07:52] EP 8: action=2, reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Environment: Reset
[00:07:52] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[00:07:52] RL Training - Episode 9: {'episode': 9, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.7777777777777778, 'success_rate': 0.1111111111111111}
[00:07:52] EP 9: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] RL Environment: Reset
[00:07:52] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Reward: -1
[00:07:52] RL Training - Episode 10: {'episode': 10, 'action': 1, 'reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.8, 'success_rate': 0.1}
[00:07:52] EP 10: action=1, reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:07:52] Avg reward: -0.80, Success rate: 10.00%
[00:07:52] RL Training Summary: {'status': 'training_completed', 'avg_reward': -0.8, 'success_rate': 0.1, 'episodes': 10, 'total_reward': -8}
[00:07:52] RL Agent: Training complete. Metrics: {'avg_reward': -0.8, 'success_rate': 0.1, 'episodes': 10}
[00:07:52] RL Agent: Process complete.
[00:07:52] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[00:07:52] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[00:07:52] Orchestrator: Run complete.
[00:09:53] Orchestrator: Starting run for case: data/inputs/ahmedabad_residential.json
[00:09:53] InputAgent: Loading case from data\inputs\ahmedabad_residential.json
[00:09:53] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'road_width', 'plot_size', 'expected_rule_path']
[00:09:53] Would download: https://example.com/regulations.pdf
[00:09:53] Parsing (stub): rules_kb/sample_rules.json
[00:09:53] Classification Agent: Starting to select applicable rules.
[00:09:53] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:09:53] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:09:53] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:09:53] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:09:53] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:09:53] Classification Agent: SELECTED rule AHM-DCR-SETBACK
[00:09:53] Classification Agent: SELECTED rule AHM-DCR-COVERAGE
[00:09:53] Classification Agent: SELECTED rule AHM-DCR-FAR
[00:09:53] Classification Agent: Found 3 applicable rules.
[00:09:53] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=Ahmedabad, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, SELECTED rule AHM-DCR-SETBACK: Condition match: city=Ahmedabad, Condition match: plot_type=residential, SELECTED rule AHM-DCR-COVERAGE: Condition match: city=Ahmedabad, Condition match: plot_type=residential, SELECTED rule AHM-DCR-FAR: Condition match: city=Ahmedabad, Condition match: plot_type=residential
[00:09:53] Calculation Agent: Starting computations.
[00:09:53] Calculation Agent: Processing case with plot size: 600 sqm
[00:09:53] Calculation Agent: Location is urban
[00:09:53] Calculation Agent: Evaluating setback for rule AHM-DCR-SETBACK with road width 12
[00:09:53] Calculation Agent: Applied setback rule: road_width >= 10 -> setback = 3.0
[00:09:53] Calculation Agent: Coverage for urban is 0.5 (from rule AHM-DCR-COVERAGE)
[00:09:53] Calculation Agent: FAR for urban is 2.0 (from rule AHM-DCR-FAR)
[00:09:53] Calculation Agent: No additional FAR rule found
[00:09:53] Calculation Agent: Max footprint: 300.0 sqm, Total floor area: 1200.0 sqm
[00:09:53] Calculation Agent: Finished. Total floor area: 1200.0 sqm.
[00:09:53] RL Agent: Starting RL decision process.
[00:09:53] RL Agent: Expected rule path: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Agent: Candidate 1 (expected): ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Agent: Candidate 2 (reversed): ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK']
[00:09:53] RL Agent: Candidate 3 (rotated): ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK']
[00:09:53] RL Agent: Initializing RL environment and training...
[00:09:53] RL Environment: Initialized with 3 candidate paths
[00:09:53] RL Environment: Candidate 0: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Environment: Candidate 1: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK']
[00:09:53] RL Environment: Candidate 2: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK']
[00:09:53] Training RL (random policy) for 10 episodes...
[00:09:53] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:09:53] RL Environment: Reset
[00:09:53] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Reward: -1
[00:09:53] RL Training - Episode 1: {'episode': 1, 'action': 2, 'reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:09:53] EP 1: action=2, reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Environment: Reset
[00:09:53] RL Environment: Action 1 -> Chosen: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Reward: -1
[00:09:53] RL Training - Episode 2: {'episode': 2, 'action': 1, 'reward': -1, 'chosen_path': ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:09:53] EP 2: action=1, reward=-1, chosen=['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Environment: Reset
[00:09:53] RL Environment: Action 1 -> Chosen: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Reward: -1
[00:09:53] RL Training - Episode 3: {'episode': 3, 'action': 1, 'reward': -1, 'chosen_path': ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:09:53] EP 3: action=1, reward=-1, chosen=['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Environment: Reset
[00:09:53] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Reward: -1
[00:09:53] RL Training - Episode 4: {'episode': 4, 'action': 2, 'reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:09:53] EP 4: action=2, reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Environment: Reset
[00:09:53] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Reward: 1
[00:09:53] RL Training - Episode 5: {'episode': 5, 'action': 0, 'reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.6, 'success_rate': 0.2}
[00:09:53] EP 5: action=0, reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Environment: Reset
[00:09:53] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Reward: -1
[00:09:53] RL Training - Episode 6: {'episode': 6, 'action': 2, 'reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.6666666666666666, 'success_rate': 0.16666666666666666}
[00:09:53] EP 6: action=2, reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Environment: Reset
[00:09:53] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Reward: -1
[00:09:53] RL Training - Episode 7: {'episode': 7, 'action': 2, 'reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.7142857142857143, 'success_rate': 0.14285714285714285}
[00:09:53] EP 7: action=2, reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Environment: Reset
[00:09:53] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Reward: -1
[00:09:53] RL Training - Episode 8: {'episode': 8, 'action': 2, 'reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.75, 'success_rate': 0.125}
[00:09:53] EP 8: action=2, reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Environment: Reset
[00:09:53] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Reward: -1
[00:09:53] RL Training - Episode 9: {'episode': 9, 'action': 2, 'reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.7777777777777778, 'success_rate': 0.1111111111111111}
[00:09:53] EP 9: action=2, reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] RL Environment: Reset
[00:09:53] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Reward: -1
[00:09:53] RL Training - Episode 10: {'episode': 10, 'action': 2, 'reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.8, 'success_rate': 0.1}
[00:09:53] EP 10: action=2, reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:09:53] Avg reward: -0.80, Success rate: 10.00%
[00:09:53] RL Training Summary: {'status': 'training_completed', 'avg_reward': -0.8, 'success_rate': 0.1, 'episodes': 10, 'total_reward': -8}
[00:09:53] RL Agent: Training complete. Metrics: {'avg_reward': -0.8, 'success_rate': 0.1, 'episodes': 10}
[00:09:53] RL Agent: Process complete.
[00:09:53] Orchestrator: Saved JSON report to outputs\json\ahmedabad_residential.json
[00:09:53] Orchestrator: Saved geometry to outputs\geometry\ahmedabad_residential.json
[00:09:53] Orchestrator: Run complete.
[00:09:59] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[00:09:59] InputAgent: Loading case from data\inputs\mumbai_case.json
[00:09:59] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[00:09:59] Would download: https://example.com/regulations.pdf
[00:09:59] Parsing (stub): rules_kb/sample_rules.json
[00:09:59] Classification Agent: Starting to select applicable rules.
[00:09:59] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:09:59] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:09:59] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:09:59] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:09:59] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:09:59] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:09:59] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:09:59] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:09:59] Classification Agent: Found 0 applicable rules.
[00:09:59] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=None, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[00:09:59] Calculation Agent: Starting computations.
[00:09:59] Calculation Agent: Processing case with plot size: 500 sqm
[00:09:59] Calculation Agent: Location is urban
[00:09:59] Calculation Agent: No specific setback rule found, using default: 1.5
[00:09:59] Calculation Agent: No specific coverage rule found, using default: 0.55
[00:09:59] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:09:59] Calculation Agent: No additional FAR rule found
[00:09:59] Calculation Agent: Max footprint: 275.0 sqm, Total floor area: 750.0 sqm
[00:09:59] Calculation Agent: Finished. Total floor area: 750.0 sqm.
[00:09:59] RL Agent: Starting RL decision process.
[00:09:59] RL Agent: Expected rule path: []
[00:09:59] RL Agent: Candidate 1 (expected): []
[00:09:59] RL Agent: Candidate 2 (reversed): []
[00:09:59] RL Agent: Initializing RL environment and training...
[00:09:59] RL Environment: Initialized with 2 candidate paths
[00:09:59] RL Environment: Candidate 0: []
[00:09:59] RL Environment: Candidate 1: []
[00:09:59] Training RL (random policy) for 10 episodes...
[00:09:59] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 0 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 1: {'episode': 1, 'action': 0, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 1: action=0, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 0 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 2: {'episode': 2, 'action': 0, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 2: action=0, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 3: {'episode': 3, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 3: action=1, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 4: {'episode': 4, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 4: action=1, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 0 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 5: {'episode': 5, 'action': 0, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 5: action=0, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 6: {'episode': 6, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 6: action=1, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 0 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 7: {'episode': 7, 'action': 0, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 7: action=0, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 0 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 8: {'episode': 8, 'action': 0, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 8: action=0, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 9: {'episode': 9, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 9: action=1, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 10: {'episode': 10, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 10: action=1, reward=1, chosen=[] expected=[]
[00:09:59] Avg reward: 1.00, Success rate: 100.00%
[00:09:59] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[00:09:59] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[00:09:59] RL Agent: Process complete.
[00:09:59] Orchestrator: Saved JSON report to outputs\json\mumbai_output.json
[00:09:59] Orchestrator: Saved geometry to outputs\geometry\mumbai_model.stl
[00:09:59] Orchestrator: Run complete.
[00:09:59] Orchestrator: Starting run for case: data/inputs/pune_case.json
[00:09:59] InputAgent: Loading case from data\inputs\pune_case.json
[00:09:59] InputAgent: Successfully loaded case with keys: ['city', 'type', 'plot_size']
[00:09:59] Would download: https://example.com/regulations.pdf
[00:09:59] Parsing (stub): rules_kb/sample_rules.json
[00:09:59] Classification Agent: Starting to select applicable rules.
[00:09:59] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:09:59] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:09:59] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:09:59] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:09:59] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:09:59] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:09:59] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:09:59] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:09:59] Classification Agent: Found 0 applicable rules.
[00:09:59] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=Pune, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[00:09:59] Calculation Agent: Starting computations.
[00:09:59] Calculation Agent: Processing case with plot size: 400 sqm
[00:09:59] Calculation Agent: Location is default
[00:09:59] Calculation Agent: No specific setback rule found, using default: 1.5
[00:09:59] Calculation Agent: No specific coverage rule found, using default: 0.55
[00:09:59] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:09:59] Calculation Agent: No additional FAR rule found
[00:09:59] Calculation Agent: Max footprint: 220.00000000000003 sqm, Total floor area: 600.0 sqm
[00:09:59] Calculation Agent: Finished. Total floor area: 600.0 sqm.
[00:09:59] RL Agent: Starting RL decision process.
[00:09:59] RL Agent: Expected rule path: []
[00:09:59] RL Agent: Candidate 1 (expected): []
[00:09:59] RL Agent: Candidate 2 (reversed): []
[00:09:59] RL Agent: Initializing RL environment and training...
[00:09:59] RL Environment: Initialized with 2 candidate paths
[00:09:59] RL Environment: Candidate 0: []
[00:09:59] RL Environment: Candidate 1: []
[00:09:59] Training RL (random policy) for 10 episodes...
[00:09:59] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 1: {'episode': 1, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 1: action=1, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 2: {'episode': 2, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 2: action=1, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 3: {'episode': 3, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 3: action=1, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 4: {'episode': 4, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 4: action=1, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 5: {'episode': 5, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 5: action=1, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 0 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 6: {'episode': 6, 'action': 0, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 6: action=0, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 1 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 7: {'episode': 7, 'action': 1, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 7: action=1, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 0 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 8: {'episode': 8, 'action': 0, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 8: action=0, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 0 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 9: {'episode': 9, 'action': 0, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 9: action=0, reward=1, chosen=[] expected=[]
[00:09:59] RL Environment: Reset
[00:09:59] RL Environment: Action 0 -> Chosen: [], Expected: [], Reward: 1
[00:09:59] RL Training - Episode 10: {'episode': 10, 'action': 0, 'reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:09:59] EP 10: action=0, reward=1, chosen=[] expected=[]
[00:09:59] Avg reward: 1.00, Success rate: 100.00%
[00:09:59] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[00:09:59] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[00:09:59] RL Agent: Process complete.
[00:09:59] Orchestrator: Saved JSON report to outputs\json\pune_output.json
[00:09:59] Orchestrator: Saved geometry to outputs\geometry\pune_model.stl
[00:09:59] Orchestrator: Run complete.
[00:19:44] Feedback received for case test: up
[00:19:49] Feedback received for case test: up
[00:19:49] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[00:19:49] InputAgent: Loading case from data\inputs\mumbai_case.json
[00:19:49] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[00:19:49] Would download: https://example.com/regulations.pdf
[00:19:49] Parsing (stub): rules_kb/sample_rules.json
[00:19:49] Classification Agent: Starting to select applicable rules.
[00:19:49] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:19:49] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:19:49] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:19:49] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:19:49] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:19:49] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:19:49] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:19:49] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:19:49] Classification Agent: Found 0 applicable rules.
[00:19:49] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=None, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[00:19:49] Calculation Agent: Starting computations.
[00:19:49] Calculation Agent: Processing case with plot size: 500 sqm
[00:19:49] Calculation Agent: Location is urban
[00:19:49] Calculation Agent: No specific setback rule found, using default: 1.5
[00:19:49] Calculation Agent: No specific coverage rule found, using default: 0.55
[00:19:49] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:19:49] Calculation Agent: No additional FAR rule found
[00:19:49] Calculation Agent: Max footprint: 275.0 sqm, Total floor area: 750.0 sqm
[00:19:49] Calculation Agent: Finished. Total floor area: 750.0 sqm.
[00:19:49] RL Agent: Starting RL decision process.
[00:19:49] RL Agent: Expected rule path: []
[00:19:49] RL Agent: Candidate 1 (expected): []
[00:19:49] RL Agent: Candidate 2 (reversed): []
[00:19:49] RL Agent: Initializing RL environment and training...
[00:19:49] RL Environment: Initialized with 2 candidate paths
[00:19:49] RL Environment: Candidate 0: []
[00:19:49] RL Environment: Candidate 1: []
[00:19:49] Training RL (random policy) for 10 episodes...
[00:19:49] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case mumbai-001: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 1: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case mumbai-001: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 2: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case mumbai-001: 0
[00:19:49] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 3: {'episode': 3, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 3: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case mumbai-001: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 4: {'episode': 4, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 4: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case mumbai-001: 0
[00:19:49] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case mumbai-001: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 6: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case mumbai-001: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 7: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case mumbai-001: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 8: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case mumbai-001: 0
[00:19:49] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 9: {'episode': 9, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 9: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case mumbai-001: 0
[00:19:49] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] Avg reward: 1.00, Success rate: 100.00%
[00:19:49] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[00:19:49] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[00:19:49] RL Agent: Process complete.
[00:19:49] Orchestrator: Saved JSON report to outputs\json\mumbai_output.json
[00:19:49] Orchestrator: Saved geometry to outputs\geometry\mumbai_model.stl
[00:19:49] Orchestrator: Run complete.
[00:19:49] Orchestrator: Starting run for case: data/inputs/pune_case.json
[00:19:49] InputAgent: Loading case from data\inputs\pune_case.json
[00:19:49] InputAgent: Successfully loaded case with keys: ['city', 'type', 'plot_size']
[00:19:49] Would download: https://example.com/regulations.pdf
[00:19:49] Parsing (stub): rules_kb/sample_rules.json
[00:19:49] Classification Agent: Starting to select applicable rules.
[00:19:49] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:19:49] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:19:49] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:19:49] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:19:49] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:19:49] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:19:49] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:19:49] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:19:49] Classification Agent: Found 0 applicable rules.
[00:19:49] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=Pune, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[00:19:49] Calculation Agent: Starting computations.
[00:19:49] Calculation Agent: Processing case with plot size: 400 sqm
[00:19:49] Calculation Agent: Location is default
[00:19:49] Calculation Agent: No specific setback rule found, using default: 1.5
[00:19:49] Calculation Agent: No specific coverage rule found, using default: 0.55
[00:19:49] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:19:49] Calculation Agent: No additional FAR rule found
[00:19:49] Calculation Agent: Max footprint: 220.00000000000003 sqm, Total floor area: 600.0 sqm
[00:19:49] Calculation Agent: Finished. Total floor area: 600.0 sqm.
[00:19:49] RL Agent: Starting RL decision process.
[00:19:49] RL Agent: Expected rule path: []
[00:19:49] RL Agent: Candidate 1 (expected): []
[00:19:49] RL Agent: Candidate 2 (reversed): []
[00:19:49] RL Agent: Initializing RL environment and training...
[00:19:49] RL Environment: Initialized with 2 candidate paths
[00:19:49] RL Environment: Candidate 0: []
[00:19:49] RL Environment: Candidate 1: []
[00:19:49] Training RL (random policy) for 10 episodes...
[00:19:49] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case unknown: 0
[00:19:49] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case unknown: 0
[00:19:49] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case unknown: 0
[00:19:49] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 3: {'episode': 3, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 3: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case unknown: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 4: {'episode': 4, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 4: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case unknown: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 5: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case unknown: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 6: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case unknown: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 7: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case unknown: 0
[00:19:49] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case unknown: 0
[00:19:49] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 9: {'episode': 9, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 9: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] RL Environment: Reset
[00:19:49] RL Environment: Feedback reward for case unknown: 0
[00:19:49] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:19:49] RL Training - Episode 10: {'episode': 10, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:19:49] EP 10: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:19:49] Avg reward: 1.00, Success rate: 100.00%
[00:19:49] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[00:19:49] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[00:19:49] RL Agent: Process complete.
[00:19:49] Orchestrator: Saved JSON report to outputs\json\pune_output.json
[00:19:49] Orchestrator: Saved geometry to outputs\geometry\pune_model.stl
[00:19:49] Orchestrator: Run complete.
[00:19:57] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[00:19:57] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[00:19:57] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[00:19:57] Would download: https://example.com/regulations.pdf
[00:19:57] Parsing (stub): rules_kb/sample_rules.json
[00:19:57] Classification Agent: Starting to select applicable rules.
[00:19:57] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:19:57] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[00:19:57] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[00:19:57] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[00:19:57] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[00:19:57] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:19:57] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:19:57] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:19:57] Classification Agent: Found 5 applicable rules.
[00:19:57] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[00:19:57] Calculation Agent: Starting computations.
[00:19:57] Calculation Agent: Processing case with plot size: 800 sqm
[00:19:57] Calculation Agent: Location is urban
[00:19:57] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[00:19:57] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[00:19:57] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[00:19:57] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:19:57] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[00:19:57] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[00:19:57] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[00:19:57] RL Agent: Starting RL decision process.
[00:19:57] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:19:57] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:19:57] RL Agent: Initializing RL environment and training...
[00:19:57] RL Environment: Initialized with 3 candidate paths
[00:19:57] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:19:57] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:19:57] Training RL (random policy) for 10 episodes...
[00:19:57] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:19:57] RL Environment: Reset
[00:19:57] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:19:57] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:19:57] RL Training - Episode 1: {'episode': 1, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:19:57] EP 1: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Environment: Reset
[00:19:57] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:19:57] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:19:57] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.0, 'success_rate': 0.5}
[00:19:57] EP 2: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Environment: Reset
[00:19:57] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:19:57] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:19:57] RL Training - Episode 3: {'episode': 3, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.6666666666666667, 'success_rate': 0.3333333333333333}
[00:19:57] EP 3: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Environment: Reset
[00:19:57] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:19:57] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:19:57] RL Training - Episode 4: {'episode': 4, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.5, 'success_rate': 0.25}
[00:19:57] EP 4: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Environment: Reset
[00:19:57] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:19:57] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:19:57] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.8, 'success_rate': 0.4}
[00:19:57] EP 5: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Environment: Reset
[00:19:57] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:19:57] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:19:57] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.6666666666666667, 'success_rate': 0.3333333333333333}
[00:19:57] EP 6: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Environment: Reset
[00:19:57] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:19:57] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:19:57] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.5714285714285714, 'success_rate': 0.2857142857142857}
[00:19:57] EP 7: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Environment: Reset
[00:19:57] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:19:57] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:19:57] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.5, 'success_rate': 0.25}
[00:19:57] EP 8: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Environment: Reset
[00:19:57] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:19:57] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:19:57] RL Training - Episode 9: {'episode': 9, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.4444444444444444, 'success_rate': 0.2222222222222222}
[00:19:57] EP 9: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] RL Environment: Reset
[00:19:57] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:19:57] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:19:57] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.6, 'success_rate': 0.3}
[00:19:57] EP 10: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:19:57] Avg reward: 1.60, Success rate: 30.00%
[00:19:57] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.6, 'success_rate': 0.3, 'episodes': 10, 'total_reward': 16}
[00:19:57] RL Agent: Training complete. Metrics: {'avg_reward': 1.6, 'success_rate': 0.3, 'episodes': 10}
[00:19:57] RL Agent: Process complete.
[00:19:57] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[00:19:57] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[00:19:57] Orchestrator: Run complete.
[00:20:00] Orchestrator: Starting run for case: data/inputs/mmumbai_redevelopment.json
[00:20:00] InputAgent: Loading case from data\inputs\mmumbai_redevelopment.json
[00:20:00] Orchestrator: ERROR while running case data/inputs/mmumbai_redevelopment.json: Input file not found: data\inputs\mmumbai_redevelopment.json
[00:20:04] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[00:20:04] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[00:20:04] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[00:20:04] Would download: https://example.com/regulations.pdf
[00:20:04] Parsing (stub): rules_kb/sample_rules.json
[00:20:04] Classification Agent: Starting to select applicable rules.
[00:20:04] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:20:04] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[00:20:04] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[00:20:04] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[00:20:04] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[00:20:04] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:20:04] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:20:04] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:20:04] Classification Agent: Found 5 applicable rules.
[00:20:04] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[00:20:04] Calculation Agent: Starting computations.
[00:20:04] Calculation Agent: Processing case with plot size: 800 sqm
[00:20:04] Calculation Agent: Location is urban
[00:20:04] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[00:20:04] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[00:20:04] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[00:20:04] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:20:04] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[00:20:04] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[00:20:04] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[00:20:04] RL Agent: Starting RL decision process.
[00:20:04] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:20:04] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:20:04] RL Agent: Initializing RL environment and training...
[00:20:04] RL Environment: Initialized with 3 candidate paths
[00:20:04] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:20:04] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:20:04] Training RL (random policy) for 10 episodes...
[00:20:04] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:20:04] RL Environment: Reset
[00:20:04] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:04] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:20:04] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 3.0, 'success_rate': 1.0}
[00:20:04] EP 1: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Environment: Reset
[00:20:04] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:04] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:04] RL Training - Episode 2: {'episode': 2, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.0, 'success_rate': 0.5}
[00:20:04] EP 2: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Environment: Reset
[00:20:04] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:04] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:04] RL Training - Episode 3: {'episode': 3, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.6666666666666667, 'success_rate': 0.3333333333333333}
[00:20:04] EP 3: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Environment: Reset
[00:20:04] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:04] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:20:04] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.0, 'success_rate': 0.5}
[00:20:04] EP 4: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Environment: Reset
[00:20:04] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:04] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:20:04] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.2, 'success_rate': 0.6}
[00:20:04] EP 5: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Environment: Reset
[00:20:04] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:04] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:20:04] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.3333333333333335, 'success_rate': 0.6666666666666666}
[00:20:04] EP 6: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Environment: Reset
[00:20:04] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:04] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:04] RL Training - Episode 7: {'episode': 7, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.142857142857143, 'success_rate': 0.5714285714285714}
[00:20:04] EP 7: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Environment: Reset
[00:20:04] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:04] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:04] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.0, 'success_rate': 0.5}
[00:20:04] EP 8: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Environment: Reset
[00:20:04] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:04] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:04] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.8888888888888888, 'success_rate': 0.4444444444444444}
[00:20:04] EP 9: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] RL Environment: Reset
[00:20:04] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:04] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:20:04] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.0, 'success_rate': 0.5}
[00:20:04] EP 10: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:04] Avg reward: 2.00, Success rate: 50.00%
[00:20:04] RL Training Summary: {'status': 'training_completed', 'avg_reward': 2.0, 'success_rate': 0.5, 'episodes': 10, 'total_reward': 20}
[00:20:04] RL Agent: Training complete. Metrics: {'avg_reward': 2.0, 'success_rate': 0.5, 'episodes': 10}
[00:20:04] RL Agent: Process complete.
[00:20:04] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[00:20:04] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[00:20:04] Orchestrator: Run complete.
[00:20:46] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[00:20:46] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[00:20:46] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[00:20:46] Would download: https://example.com/regulations.pdf
[00:20:46] Parsing (stub): rules_kb/sample_rules.json
[00:20:46] Classification Agent: Starting to select applicable rules.
[00:20:46] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:20:46] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[00:20:46] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[00:20:46] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[00:20:46] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[00:20:46] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:20:46] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:20:46] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:20:46] Classification Agent: Found 5 applicable rules.
[00:20:46] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[00:20:46] Calculation Agent: Starting computations.
[00:20:46] Calculation Agent: Processing case with plot size: 800 sqm
[00:20:46] Calculation Agent: Location is urban
[00:20:46] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[00:20:46] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[00:20:46] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[00:20:46] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:20:46] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[00:20:46] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[00:20:46] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[00:20:46] RL Agent: Starting RL decision process.
[00:20:46] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:20:46] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:20:46] RL Agent: Initializing RL environment and training...
[00:20:46] RL Environment: Initialized with 3 candidate paths
[00:20:46] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:20:46] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:20:46] Training RL (random policy) for 10 episodes...
[00:20:46] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:20:46] RL Environment: Reset
[00:20:46] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:46] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:46] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:20:46] EP 1: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Environment: Reset
[00:20:46] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:46] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:46] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:20:46] EP 2: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Environment: Reset
[00:20:46] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:46] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:46] RL Training - Episode 3: {'episode': 3, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:20:46] EP 3: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Environment: Reset
[00:20:46] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:46] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:46] RL Training - Episode 4: {'episode': 4, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:20:46] EP 4: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Environment: Reset
[00:20:46] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:46] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:46] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:20:46] EP 5: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Environment: Reset
[00:20:46] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:46] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:46] RL Training - Episode 6: {'episode': 6, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:20:46] EP 6: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Environment: Reset
[00:20:46] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:46] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:46] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:20:46] EP 7: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Environment: Reset
[00:20:46] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:46] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:46] RL Training - Episode 8: {'episode': 8, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:20:46] EP 8: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Environment: Reset
[00:20:46] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:46] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:46] RL Training - Episode 9: {'episode': 9, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:20:46] EP 9: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] RL Environment: Reset
[00:20:46] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:20:46] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:20:46] RL Training - Episode 10: {'episode': 10, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:20:46] EP 10: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:20:46] Avg reward: 1.00, Success rate: 0.00%
[00:20:46] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 0.0, 'episodes': 10, 'total_reward': 10}
[00:20:46] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 0.0, 'episodes': 10}
[00:20:46] RL Agent: Process complete.
[00:20:46] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[00:20:46] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[00:20:46] Orchestrator: Run complete.
[00:21:30] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[00:21:30] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[00:21:30] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[00:21:30] Would download: https://example.com/regulations.pdf
[00:21:30] Parsing (stub): rules_kb/sample_rules.json
[00:21:30] Classification Agent: Starting to select applicable rules.
[00:21:30] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:21:30] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[00:21:30] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[00:21:30] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[00:21:30] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[00:21:30] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:21:30] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:21:30] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:21:30] Classification Agent: Found 5 applicable rules.
[00:21:30] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[00:21:30] Calculation Agent: Starting computations.
[00:21:30] Calculation Agent: Processing case with plot size: 800 sqm
[00:21:30] Calculation Agent: Location is urban
[00:21:30] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[00:21:30] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[00:21:30] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[00:21:30] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:21:30] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[00:21:30] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[00:21:30] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[00:21:30] RL Agent: Starting RL decision process.
[00:21:30] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:21:30] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:21:30] RL Agent: Initializing RL environment and training...
[00:21:30] RL Environment: Initialized with 3 candidate paths
[00:21:30] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:21:30] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:21:30] Training RL (random policy) for 10 episodes...
[00:21:30] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:21:30] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:21:30] RL Training - Episode 1: {'episode': 1, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 0.0}
[00:21:30] EP 1: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:21:30] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:21:30] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.0, 'success_rate': 0.5}
[00:21:30] EP 2: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:21:30] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:21:30] RL Training - Episode 3: {'episode': 3, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.6666666666666667, 'success_rate': 0.3333333333333333}
[00:21:30] EP 3: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:21:30] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:21:30] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.0, 'success_rate': 0.5}
[00:21:30] EP 4: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:21:30] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:21:30] RL Training - Episode 5: {'episode': 5, 'action': 2, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.8, 'success_rate': 0.4}
[00:21:30] EP 5: action=2, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:21:30] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:21:30] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.0, 'success_rate': 0.5}
[00:21:30] EP 6: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:21:30] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:21:30] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.142857142857143, 'success_rate': 0.5714285714285714}
[00:21:30] EP 7: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:21:30] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 2, Total Reward: 3
[00:21:30] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 2, 'total_reward': 3, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.25, 'success_rate': 0.625}
[00:21:30] EP 8: action=0, base_reward=1, feedback_reward=2, total_reward=3, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:21:30] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:21:30] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.111111111111111, 'success_rate': 0.5555555555555556}
[00:21:30] EP 9: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case mumbai-redev-001: 2
[00:21:30] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 2, Total Reward: 1
[00:21:30] RL Training - Episode 10: {'episode': 10, 'action': 1, 'base_reward': -1, 'feedback_reward': 2, 'total_reward': 1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 2.0, 'success_rate': 0.5}
[00:21:30] EP 10: action=1, base_reward=-1, feedback_reward=2, total_reward=1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:21:30] Avg reward: 2.00, Success rate: 50.00%
[00:21:30] RL Training Summary: {'status': 'training_completed', 'avg_reward': 2.0, 'success_rate': 0.5, 'episodes': 10, 'total_reward': 20}
[00:21:30] RL Agent: Training complete. Metrics: {'avg_reward': 2.0, 'success_rate': 0.5, 'episodes': 10}
[00:21:30] RL Agent: Process complete.
[00:21:30] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[00:21:30] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[00:21:30] Orchestrator: Run complete.
[00:21:30] Orchestrator: Starting run for case: data/inputs/ahmedabad_residential.json
[00:21:30] InputAgent: Loading case from data\inputs\ahmedabad_residential.json
[00:21:30] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'road_width', 'plot_size', 'expected_rule_path']
[00:21:30] Would download: https://example.com/regulations.pdf
[00:21:30] Parsing (stub): rules_kb/sample_rules.json
[00:21:30] Classification Agent: Starting to select applicable rules.
[00:21:30] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:21:30] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:21:30] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:21:30] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:21:30] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:21:30] Classification Agent: SELECTED rule AHM-DCR-SETBACK
[00:21:30] Classification Agent: SELECTED rule AHM-DCR-COVERAGE
[00:21:30] Classification Agent: SELECTED rule AHM-DCR-FAR
[00:21:30] Classification Agent: Found 3 applicable rules.
[00:21:30] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=Ahmedabad, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, SELECTED rule AHM-DCR-SETBACK: Condition match: city=Ahmedabad, Condition match: plot_type=residential, SELECTED rule AHM-DCR-COVERAGE: Condition match: city=Ahmedabad, Condition match: plot_type=residential, SELECTED rule AHM-DCR-FAR: Condition match: city=Ahmedabad, Condition match: plot_type=residential
[00:21:30] Calculation Agent: Starting computations.
[00:21:30] Calculation Agent: Processing case with plot size: 600 sqm
[00:21:30] Calculation Agent: Location is urban
[00:21:30] Calculation Agent: Evaluating setback for rule AHM-DCR-SETBACK with road width 12
[00:21:30] Calculation Agent: Applied setback rule: road_width >= 10 -> setback = 3.0
[00:21:30] Calculation Agent: Coverage for urban is 0.5 (from rule AHM-DCR-COVERAGE)
[00:21:30] Calculation Agent: FAR for urban is 2.0 (from rule AHM-DCR-FAR)
[00:21:30] Calculation Agent: No additional FAR rule found
[00:21:30] Calculation Agent: Max footprint: 300.0 sqm, Total floor area: 1200.0 sqm
[00:21:30] Calculation Agent: Finished. Total floor area: 1200.0 sqm.
[00:21:30] RL Agent: Starting RL decision process.
[00:21:30] RL Agent: Expected rule path: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Agent: Candidate 1 (expected): ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Agent: Candidate 2 (reversed): ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK']
[00:21:30] RL Agent: Candidate 3 (rotated): ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK']
[00:21:30] RL Agent: Initializing RL environment and training...
[00:21:30] RL Environment: Initialized with 3 candidate paths
[00:21:30] RL Environment: Candidate 0: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Environment: Candidate 1: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK']
[00:21:30] RL Environment: Candidate 2: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK']
[00:21:30] Training RL (random policy) for 10 episodes...
[00:21:30] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[00:21:30] RL Environment: Action 1 -> Chosen: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:21:30] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:21:30] EP 1: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[00:21:30] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:30] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[00:21:30] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[00:21:30] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:21:30] RL Training - Episode 3: {'episode': 3, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[00:21:30] EP 3: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[00:21:30] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:21:30] RL Training - Episode 4: {'episode': 4, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.5, 'success_rate': 0.25}
[00:21:30] EP 4: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[00:21:30] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:30] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.2, 'success_rate': 0.4}
[00:21:30] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[00:21:30] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:30] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[00:21:30] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[00:21:30] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:21:30] RL Training - Episode 7: {'episode': 7, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.14285714285714285, 'success_rate': 0.42857142857142855}
[00:21:30] EP 7: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[00:21:30] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:30] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[00:21:30] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[00:21:30] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:30] RL Training - Episode 9: {'episode': 9, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.1111111111111111, 'success_rate': 0.5555555555555556}
[00:21:30] EP 9: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] RL Environment: Reset
[00:21:30] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[00:21:30] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:30] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.2, 'success_rate': 0.6}
[00:21:30] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[00:21:30] Avg reward: 0.20, Success rate: 60.00%
[00:21:30] RL Training Summary: {'status': 'training_completed', 'avg_reward': 0.2, 'success_rate': 0.6, 'episodes': 10, 'total_reward': 2}
[00:21:30] RL Agent: Training complete. Metrics: {'avg_reward': 0.2, 'success_rate': 0.6, 'episodes': 10}
[00:21:30] RL Agent: Process complete.
[00:21:30] Orchestrator: Saved JSON report to outputs\json\ahmedabad_residential.json
[00:21:30] Orchestrator: Saved geometry to outputs\geometry\ahmedabad_residential.json
[00:21:30] Orchestrator: Run complete.
[00:21:30] Feedback received for case test: up
[00:21:30] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[00:21:30] InputAgent: Loading case from data\inputs\mumbai_case.json
[00:21:30] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[00:21:30] Would download: https://example.com/regulations.pdf
[00:21:30] Parsing (stub): rules_kb/sample_rules.json
[00:21:30] Classification Agent: Starting to select applicable rules.
[00:21:30] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:21:30] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:21:30] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:21:30] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:21:30] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:21:30] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:21:30] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:21:30] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:21:30] Classification Agent: Found 0 applicable rules.
[00:21:30] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=None, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[00:21:30] Calculation Agent: Starting computations.
[00:21:30] Calculation Agent: Processing case with plot size: 500 sqm
[00:21:30] Calculation Agent: Location is urban
[00:21:30] Calculation Agent: No specific setback rule found, using default: 1.5
[00:21:30] Calculation Agent: No specific coverage rule found, using default: 0.55
[00:21:30] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:21:30] Calculation Agent: No additional FAR rule found
[00:21:30] Calculation Agent: Max footprint: 275.0 sqm, Total floor area: 750.0 sqm
[00:21:30] Calculation Agent: Finished. Total floor area: 750.0 sqm.
[00:21:30] RL Agent: Starting RL decision process.
[00:21:30] RL Agent: Expected rule path: []
[00:21:30] RL Agent: Candidate 1 (expected): []
[00:21:30] RL Agent: Candidate 2 (reversed): []
[00:21:30] RL Agent: Initializing RL environment and training...
[00:21:30] RL Environment: Initialized with 2 candidate paths
[00:21:30] RL Environment: Candidate 0: []
[00:21:30] RL Environment: Candidate 1: []
[00:21:30] Training RL (random policy) for 10 episodes...
[00:21:30] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:21:30] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case mumbai-001: 0
[00:21:31] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case mumbai-001: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 2: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case mumbai-001: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 3: {'episode': 3, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 3: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case mumbai-001: 0
[00:21:31] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case mumbai-001: 0
[00:21:31] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case mumbai-001: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 6: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case mumbai-001: 0
[00:21:31] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 7: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case mumbai-001: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 8: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case mumbai-001: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 9: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case mumbai-001: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 10: {'episode': 10, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 10: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] Avg reward: 1.00, Success rate: 100.00%
[00:21:31] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[00:21:31] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[00:21:31] RL Agent: Process complete.
[00:21:31] Orchestrator: Saved JSON report to outputs\json\mumbai_output.json
[00:21:31] Orchestrator: Saved geometry to outputs\geometry\mumbai_model.stl
[00:21:31] Orchestrator: Run complete.
[00:21:31] Orchestrator: Starting run for case: data/inputs/pune_case.json
[00:21:31] InputAgent: Loading case from data\inputs\pune_case.json
[00:21:31] InputAgent: Successfully loaded case with keys: ['city', 'type', 'plot_size']
[00:21:31] Would download: https://example.com/regulations.pdf
[00:21:31] Parsing (stub): rules_kb/sample_rules.json
[00:21:31] Classification Agent: Starting to select applicable rules.
[00:21:31] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:21:31] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:21:31] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:21:31] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:21:31] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:21:31] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:21:31] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:21:31] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:21:31] Classification Agent: Found 0 applicable rules.
[00:21:31] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=Pune, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[00:21:31] Calculation Agent: Starting computations.
[00:21:31] Calculation Agent: Processing case with plot size: 400 sqm
[00:21:31] Calculation Agent: Location is default
[00:21:31] Calculation Agent: No specific setback rule found, using default: 1.5
[00:21:31] Calculation Agent: No specific coverage rule found, using default: 0.55
[00:21:31] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:21:31] Calculation Agent: No additional FAR rule found
[00:21:31] Calculation Agent: Max footprint: 220.00000000000003 sqm, Total floor area: 600.0 sqm
[00:21:31] Calculation Agent: Finished. Total floor area: 600.0 sqm.
[00:21:31] RL Agent: Starting RL decision process.
[00:21:31] RL Agent: Expected rule path: []
[00:21:31] RL Agent: Candidate 1 (expected): []
[00:21:31] RL Agent: Candidate 2 (reversed): []
[00:21:31] RL Agent: Initializing RL environment and training...
[00:21:31] RL Environment: Initialized with 2 candidate paths
[00:21:31] RL Environment: Candidate 0: []
[00:21:31] RL Environment: Candidate 1: []
[00:21:31] Training RL (random policy) for 10 episodes...
[00:21:31] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case unknown: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 1: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case unknown: 0
[00:21:31] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case unknown: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 3: {'episode': 3, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 3: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case unknown: 0
[00:21:31] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case unknown: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 5: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case unknown: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 6: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case unknown: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 7: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case unknown: 0
[00:21:31] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case unknown: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 9: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] RL Environment: Reset
[00:21:31] RL Environment: Feedback reward for case unknown: 0
[00:21:31] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:21:31] RL Training - Episode 10: {'episode': 10, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:21:31] EP 10: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:21:31] Avg reward: 1.00, Success rate: 100.00%
[00:21:31] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[00:21:31] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[00:21:31] RL Agent: Process complete.
[00:21:31] Orchestrator: Saved JSON report to outputs\json\pune_output.json
[00:21:31] Orchestrator: Saved geometry to outputs\geometry\pune_model.stl
[00:21:31] Orchestrator: Run complete.
[19:39:44] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[19:39:44] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[19:39:44] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[19:39:44] Would download: https://example.com/regulations.pdf
[19:39:44] Parsing (stub): rules_kb/sample_rules.json
[19:39:44] Classification Agent: Starting to select applicable rules.
[19:39:44] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[19:39:44] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[19:39:44] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[19:39:44] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[19:39:44] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[19:39:44] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[19:39:44] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[19:39:44] Classification Agent: REJECTED rule AHM-DCR-FAR
[19:39:44] Classification Agent: Found 5 applicable rules.
[19:39:44] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[19:39:44] Calculation Agent: Starting computations.
[19:39:44] Calculation Agent: Processing case with plot size: 800 sqm
[19:39:44] Calculation Agent: Location is urban
[19:39:44] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[19:39:44] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[19:39:44] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[19:39:44] Calculation Agent: No specific FAR rule found, using default: 1.5
[19:39:44] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[19:39:44] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[19:39:44] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[19:39:44] RL Agent: Starting RL decision process.
[19:39:44] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[19:39:44] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[19:39:44] RL Agent: Initializing RL environment and training...
[19:39:44] RL Environment: Initialized with 3 candidate paths
[19:39:44] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[19:39:44] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[19:39:44] Training RL (random policy) for 10 episodes...
[19:39:44] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[19:39:44] RL Environment: Reset
[19:39:44] RL Environment: Feedback reward for case mumbai-redev-001: 0
[19:39:44] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:44] RL Training - Episode 1: {'episode': 1, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[19:39:44] EP 1: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Environment: Reset
[19:39:44] RL Environment: Feedback reward for case mumbai-redev-001: 0
[19:39:44] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:44] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[19:39:44] EP 2: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Environment: Reset
[19:39:44] RL Environment: Feedback reward for case mumbai-redev-001: 0
[19:39:44] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:44] RL Training - Episode 3: {'episode': 3, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[19:39:44] EP 3: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Environment: Reset
[19:39:44] RL Environment: Feedback reward for case mumbai-redev-001: 0
[19:39:44] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:44] RL Training - Episode 4: {'episode': 4, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[19:39:44] EP 4: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Environment: Reset
[19:39:44] RL Environment: Feedback reward for case mumbai-redev-001: 0
[19:39:44] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:44] RL Training - Episode 5: {'episode': 5, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[19:39:44] EP 5: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Environment: Reset
[19:39:44] RL Environment: Feedback reward for case mumbai-redev-001: 0
[19:39:44] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:44] RL Training - Episode 6: {'episode': 6, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[19:39:44] EP 6: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Environment: Reset
[19:39:44] RL Environment: Feedback reward for case mumbai-redev-001: 0
[19:39:44] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:44] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[19:39:44] EP 7: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Environment: Reset
[19:39:44] RL Environment: Feedback reward for case mumbai-redev-001: 0
[19:39:44] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:44] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.75, 'success_rate': 0.125}
[19:39:44] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Environment: Reset
[19:39:44] RL Environment: Feedback reward for case mumbai-redev-001: 0
[19:39:44] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:44] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.7777777777777778, 'success_rate': 0.1111111111111111}
[19:39:44] EP 9: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] RL Environment: Reset
[19:39:44] RL Environment: Feedback reward for case mumbai-redev-001: 0
[19:39:44] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:44] RL Training - Episode 10: {'episode': 10, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.8, 'success_rate': 0.1}
[19:39:44] EP 10: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[19:39:44] Avg reward: -0.80, Success rate: 10.00%
[19:39:44] RL Training Summary: {'status': 'training_completed', 'avg_reward': -0.8, 'success_rate': 0.1, 'episodes': 10, 'total_reward': -8}
[19:39:44] RL Agent: Training complete. Metrics: {'avg_reward': -0.8, 'success_rate': 0.1, 'episodes': 10}
[19:39:44] RL Agent: Process complete.
[19:39:44] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[19:39:44] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[19:39:44] Orchestrator: Run complete.
[19:39:44] Orchestrator: Starting run for case: data/inputs/ahmedabad_residential.json
[19:39:44] InputAgent: Loading case from data\inputs\ahmedabad_residential.json
[19:39:44] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'road_width', 'plot_size', 'expected_rule_path']
[19:39:44] Would download: https://example.com/regulations.pdf
[19:39:44] Parsing (stub): rules_kb/sample_rules.json
[19:39:44] Classification Agent: Starting to select applicable rules.
[19:39:44] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[19:39:44] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[19:39:44] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[19:39:44] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[19:39:44] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[19:39:44] Classification Agent: SELECTED rule AHM-DCR-SETBACK
[19:39:44] Classification Agent: SELECTED rule AHM-DCR-COVERAGE
[19:39:44] Classification Agent: SELECTED rule AHM-DCR-FAR
[19:39:44] Classification Agent: Found 3 applicable rules.
[19:39:44] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=Ahmedabad, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, SELECTED rule AHM-DCR-SETBACK: Condition match: city=Ahmedabad, Condition match: plot_type=residential, SELECTED rule AHM-DCR-COVERAGE: Condition match: city=Ahmedabad, Condition match: plot_type=residential, SELECTED rule AHM-DCR-FAR: Condition match: city=Ahmedabad, Condition match: plot_type=residential
[19:39:44] Calculation Agent: Starting computations.
[19:39:44] Calculation Agent: Processing case with plot size: 600 sqm
[19:39:44] Calculation Agent: Location is urban
[19:39:44] Calculation Agent: Evaluating setback for rule AHM-DCR-SETBACK with road width 12
[19:39:44] Calculation Agent: Applied setback rule: road_width >= 10 -> setback = 3.0
[19:39:44] Calculation Agent: Coverage for urban is 0.5 (from rule AHM-DCR-COVERAGE)
[19:39:45] Calculation Agent: FAR for urban is 2.0 (from rule AHM-DCR-FAR)
[19:39:45] Calculation Agent: No additional FAR rule found
[19:39:45] Calculation Agent: Max footprint: 300.0 sqm, Total floor area: 1200.0 sqm
[19:39:45] Calculation Agent: Finished. Total floor area: 1200.0 sqm.
[19:39:45] RL Agent: Starting RL decision process.
[19:39:45] RL Agent: Expected rule path: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Agent: Candidate 1 (expected): ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Agent: Candidate 2 (reversed): ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK']
[19:39:45] RL Agent: Candidate 3 (rotated): ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK']
[19:39:45] RL Agent: Initializing RL environment and training...
[19:39:45] RL Environment: Initialized with 3 candidate paths
[19:39:45] RL Environment: Candidate 0: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Environment: Candidate 1: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK']
[19:39:45] RL Environment: Candidate 2: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK']
[19:39:45] Training RL (random policy) for 10 episodes...
[19:39:45] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[19:39:45] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:45] RL Training - Episode 2: {'episode': 2, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[19:39:45] EP 2: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 3: {'episode': 3, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.3333333333333333, 'success_rate': 0.6666666666666666}
[19:39:45] EP 3: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.5, 'success_rate': 0.75}
[19:39:45] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[19:39:45] RL Environment: Action 1 -> Chosen: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:45] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.2, 'success_rate': 0.6}
[19:39:45] EP 5: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[19:39:45] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:45] RL Training - Episode 6: {'episode': 6, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[19:39:45] EP 6: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.14285714285714285, 'success_rate': 0.5714285714285714}
[19:39:45] EP 7: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.25, 'success_rate': 0.625}
[19:39:45] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[19:39:45] RL Environment: Action 1 -> Chosen: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:45] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.1111111111111111, 'success_rate': 0.5555555555555556}
[19:39:45] EP 9: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[19:39:45] RL Environment: Action 1 -> Chosen: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[19:39:45] RL Training - Episode 10: {'episode': 10, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[19:39:45] EP 10: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[19:39:45] Avg reward: 0.00, Success rate: 50.00%
[19:39:45] RL Training Summary: {'status': 'training_completed', 'avg_reward': 0.0, 'success_rate': 0.5, 'episodes': 10, 'total_reward': 0}
[19:39:45] RL Agent: Training complete. Metrics: {'avg_reward': 0.0, 'success_rate': 0.5, 'episodes': 10}
[19:39:45] RL Agent: Process complete.
[19:39:45] Orchestrator: Saved JSON report to outputs\json\ahmedabad_residential.json
[19:39:45] Orchestrator: Saved geometry to outputs\geometry\ahmedabad_residential.json
[19:39:45] Orchestrator: Run complete.
[19:39:45] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[19:39:45] InputAgent: Loading case from data\inputs\mumbai_case.json
[19:39:45] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[19:39:45] Would download: https://example.com/regulations.pdf
[19:39:45] Parsing (stub): rules_kb/sample_rules.json
[19:39:45] Classification Agent: Starting to select applicable rules.
[19:39:45] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[19:39:45] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[19:39:45] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[19:39:45] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[19:39:45] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[19:39:45] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[19:39:45] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[19:39:45] Classification Agent: REJECTED rule AHM-DCR-FAR
[19:39:45] Classification Agent: Found 0 applicable rules.
[19:39:45] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=None, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[19:39:45] Calculation Agent: Starting computations.
[19:39:45] Calculation Agent: Processing case with plot size: 500 sqm
[19:39:45] Calculation Agent: Location is urban
[19:39:45] Calculation Agent: No specific setback rule found, using default: 1.5
[19:39:45] Calculation Agent: No specific coverage rule found, using default: 0.55
[19:39:45] Calculation Agent: No specific FAR rule found, using default: 1.5
[19:39:45] Calculation Agent: No additional FAR rule found
[19:39:45] Calculation Agent: Max footprint: 275.0 sqm, Total floor area: 750.0 sqm
[19:39:45] Calculation Agent: Finished. Total floor area: 750.0 sqm.
[19:39:45] RL Agent: Starting RL decision process.
[19:39:45] RL Agent: Expected rule path: []
[19:39:45] RL Agent: Candidate 1 (expected): []
[19:39:45] RL Agent: Candidate 2 (reversed): []
[19:39:45] RL Agent: Initializing RL environment and training...
[19:39:45] RL Environment: Initialized with 2 candidate paths
[19:39:45] RL Environment: Candidate 0: []
[19:39:45] RL Environment: Candidate 1: []
[19:39:45] Training RL (random policy) for 10 episodes...
[19:39:45] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case mumbai-001: 0
[19:39:45] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 1: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case mumbai-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case mumbai-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 3: {'episode': 3, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 3: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case mumbai-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case mumbai-001: 0
[19:39:45] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 5: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case mumbai-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case mumbai-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 7: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case mumbai-001: 0
[19:39:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case mumbai-001: 0
[19:39:45] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 9: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:45] RL Environment: Reset
[19:39:45] RL Environment: Feedback reward for case mumbai-001: 0
[19:39:45] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:45] RL Training - Episode 10: {'episode': 10, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:45] EP 10: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:45] Avg reward: 1.00, Success rate: 100.00%
[19:39:45] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[19:39:45] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[19:39:45] RL Agent: Process complete.
[19:39:45] Orchestrator: Saved JSON report to outputs\json\mumbai_output.json
[19:39:45] Orchestrator: Saved geometry to outputs\geometry\mumbai_model.stl
[19:39:45] Orchestrator: Run complete.
[19:39:46] Orchestrator: Starting run for case: data/inputs/pune_case.json
[19:39:46] InputAgent: Loading case from data\inputs\pune_case.json
[19:39:46] InputAgent: Successfully loaded case with keys: ['city', 'type', 'plot_size']
[19:39:46] Would download: https://example.com/regulations.pdf
[19:39:46] Parsing (stub): rules_kb/sample_rules.json
[19:39:46] Classification Agent: Starting to select applicable rules.
[19:39:46] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[19:39:46] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[19:39:46] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[19:39:46] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[19:39:46] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[19:39:46] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[19:39:46] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[19:39:46] Classification Agent: REJECTED rule AHM-DCR-FAR
[19:39:46] Classification Agent: Found 0 applicable rules.
[19:39:46] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=Pune, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[19:39:46] Calculation Agent: Starting computations.
[19:39:46] Calculation Agent: Processing case with plot size: 400 sqm
[19:39:46] Calculation Agent: Location is default
[19:39:46] Calculation Agent: No specific setback rule found, using default: 1.5
[19:39:46] Calculation Agent: No specific coverage rule found, using default: 0.55
[19:39:46] Calculation Agent: No specific FAR rule found, using default: 1.5
[19:39:46] Calculation Agent: No additional FAR rule found
[19:39:46] Calculation Agent: Max footprint: 220.00000000000003 sqm, Total floor area: 600.0 sqm
[19:39:46] Calculation Agent: Finished. Total floor area: 600.0 sqm.
[19:39:46] RL Agent: Starting RL decision process.
[19:39:46] RL Agent: Expected rule path: []
[19:39:46] RL Agent: Candidate 1 (expected): []
[19:39:46] RL Agent: Candidate 2 (reversed): []
[19:39:46] RL Agent: Initializing RL environment and training...
[19:39:46] RL Environment: Initialized with 2 candidate paths
[19:39:46] RL Environment: Candidate 0: []
[19:39:46] RL Environment: Candidate 1: []
[19:39:46] Training RL (random policy) for 10 episodes...
[19:39:46] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[19:39:46] RL Environment: Reset
[19:39:46] RL Environment: Feedback reward for case unknown: 0
[19:39:46] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:46] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:46] EP 1: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:46] RL Environment: Reset
[19:39:46] RL Environment: Feedback reward for case unknown: 0
[19:39:46] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:46] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:46] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:46] RL Environment: Reset
[19:39:46] RL Environment: Feedback reward for case unknown: 0
[19:39:46] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:46] RL Training - Episode 3: {'episode': 3, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:46] EP 3: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:46] RL Environment: Reset
[19:39:46] RL Environment: Feedback reward for case unknown: 0
[19:39:46] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:46] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:46] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:46] RL Environment: Reset
[19:39:46] RL Environment: Feedback reward for case unknown: 0
[19:39:46] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:46] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:46] EP 5: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:46] RL Environment: Reset
[19:39:46] RL Environment: Feedback reward for case unknown: 0
[19:39:46] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:46] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:46] EP 6: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:46] RL Environment: Reset
[19:39:46] RL Environment: Feedback reward for case unknown: 0
[19:39:46] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:46] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:46] EP 7: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:46] RL Environment: Reset
[19:39:46] RL Environment: Feedback reward for case unknown: 0
[19:39:46] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:46] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:46] EP 8: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:46] RL Environment: Reset
[19:39:46] RL Environment: Feedback reward for case unknown: 0
[19:39:46] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:46] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:46] EP 9: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:46] RL Environment: Reset
[19:39:46] RL Environment: Feedback reward for case unknown: 0
[19:39:46] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[19:39:46] RL Training - Episode 10: {'episode': 10, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[19:39:46] EP 10: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[19:39:46] Avg reward: 1.00, Success rate: 100.00%
[19:39:46] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[19:39:46] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[19:39:46] RL Agent: Process complete.
[19:39:46] Orchestrator: Saved JSON report to outputs\json\pune_output.json
[19:39:46] Orchestrator: Saved geometry to outputs\geometry\pune_model.stl
[19:39:46] Orchestrator: Run complete.
[20:12:36] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[20:12:36] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[20:12:36] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[20:12:36] Would download: https://example.com/regulations.pdf
[20:12:36] Parsing (stub): rules_kb/sample_rules.json
[20:12:36] Classification Agent: Starting to select applicable rules.
[20:12:36] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[20:12:36] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[20:12:36] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[20:12:36] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[20:12:36] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[20:12:36] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[20:12:36] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[20:12:36] Classification Agent: REJECTED rule AHM-DCR-FAR
[20:12:36] Classification Agent: Found 5 applicable rules.
[20:12:36] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[20:12:36] Calculation Agent: Starting computations.
[20:12:36] Calculation Agent: Processing case with plot size: 800 sqm
[20:12:36] Calculation Agent: Location is urban
[20:12:36] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[20:12:36] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[20:12:36] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[20:12:36] Calculation Agent: No specific FAR rule found, using default: 1.5
[20:12:36] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[20:12:36] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[20:12:36] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[20:12:36] RL Agent: Starting RL decision process.
[20:12:36] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[20:12:36] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[20:12:36] RL Agent: Initializing RL environment and training...
[20:12:36] RL Environment: Initialized with 3 candidate paths
[20:12:36] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[20:12:36] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[20:12:36] Training RL (random policy) for 10 episodes...
[20:12:36] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[20:12:36] RL Environment: Reset
[20:12:36] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:12:36] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:12:36] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[20:12:36] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Environment: Reset
[20:12:36] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:12:36] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[20:12:36] RL Training - Episode 2: {'episode': 2, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[20:12:36] EP 2: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Environment: Reset
[20:12:36] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:12:36] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[20:12:36] RL Training - Episode 3: {'episode': 3, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[20:12:36] EP 3: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Environment: Reset
[20:12:36] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:12:36] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:12:36] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[20:12:36] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Environment: Reset
[20:12:36] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:12:36] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[20:12:36] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.2, 'success_rate': 0.4}
[20:12:36] EP 5: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Environment: Reset
[20:12:36] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:12:36] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[20:12:36] RL Training - Episode 6: {'episode': 6, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[20:12:36] EP 6: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Environment: Reset
[20:12:36] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:12:36] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:12:36] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.14285714285714285, 'success_rate': 0.42857142857142855}
[20:12:36] EP 7: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Environment: Reset
[20:12:36] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:12:36] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[20:12:36] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.25, 'success_rate': 0.375}
[20:12:36] EP 8: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Environment: Reset
[20:12:36] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:12:36] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:12:36] RL Training - Episode 9: {'episode': 9, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.1111111111111111, 'success_rate': 0.4444444444444444}
[20:12:36] EP 9: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] RL Environment: Reset
[20:12:36] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:12:36] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:12:36] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[20:12:36] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:12:36] Avg reward: 0.00, Success rate: 50.00%
[20:12:36] RL Training Summary: {'status': 'training_completed', 'avg_reward': 0.0, 'success_rate': 0.5, 'episodes': 10, 'total_reward': 0}
[20:12:36] RL Agent: Training complete. Metrics: {'avg_reward': 0.0, 'success_rate': 0.5, 'episodes': 10}
[20:12:36] RL Agent: Process complete.
[20:12:36] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[20:12:36] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[20:12:36] Orchestrator: Run complete.
[20:15:25] Feedback received for case test-case-001: up
[20:15:39] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[20:15:39] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[20:15:39] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[20:15:39] Would download: https://example.com/regulations.pdf
[20:15:39] Parsing (stub): rules_kb/sample_rules.json
[20:15:39] Classification Agent: Starting to select applicable rules.
[20:15:39] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[20:15:39] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[20:15:39] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[20:15:39] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[20:15:39] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[20:15:39] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[20:15:39] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[20:15:39] Classification Agent: REJECTED rule AHM-DCR-FAR
[20:15:39] Classification Agent: Found 5 applicable rules.
[20:15:39] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[20:15:39] Calculation Agent: Starting computations.
[20:15:39] Calculation Agent: Processing case with plot size: 800 sqm
[20:15:39] Calculation Agent: Location is urban
[20:15:39] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[20:15:39] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[20:15:39] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[20:15:39] Calculation Agent: No specific FAR rule found, using default: 1.5
[20:15:39] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[20:15:39] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[20:15:39] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[20:15:39] RL Agent: Starting RL decision process.
[20:15:39] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[20:15:39] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[20:15:39] RL Agent: Initializing RL environment and training...
[20:15:39] RL Environment: Initialized with 3 candidate paths
[20:15:39] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[20:15:39] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[20:15:39] Training RL (random policy) for 10 episodes...
[20:15:39] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[20:15:39] RL Environment: Reset
[20:15:39] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:15:39] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[20:15:39] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[20:15:39] EP 1: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Environment: Reset
[20:15:39] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:15:39] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:15:39] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[20:15:39] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Environment: Reset
[20:15:39] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:15:39] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[20:15:39] RL Training - Episode 3: {'episode': 3, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[20:15:39] EP 3: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Environment: Reset
[20:15:39] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:15:39] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:15:39] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[20:15:39] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Environment: Reset
[20:15:39] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:15:39] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:15:39] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.2, 'success_rate': 0.6}
[20:15:39] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Environment: Reset
[20:15:39] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:15:39] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:15:39] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.3333333333333333, 'success_rate': 0.6666666666666666}
[20:15:39] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Environment: Reset
[20:15:39] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:15:39] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:15:39] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.42857142857142855, 'success_rate': 0.7142857142857143}
[20:15:39] EP 7: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Environment: Reset
[20:15:39] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:15:39] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[20:15:39] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.5, 'success_rate': 0.75}
[20:15:39] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Environment: Reset
[20:15:39] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:15:39] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[20:15:39] RL Training - Episode 9: {'episode': 9, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.3333333333333333, 'success_rate': 0.6666666666666666}
[20:15:39] EP 9: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] RL Environment: Reset
[20:15:39] RL Environment: Feedback reward for case mumbai-redev-001: 0
[20:15:39] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[20:15:39] RL Training - Episode 10: {'episode': 10, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.2, 'success_rate': 0.6}
[20:15:39] EP 10: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[20:15:39] Avg reward: 0.20, Success rate: 60.00%
[20:15:39] RL Training Summary: {'status': 'training_completed', 'avg_reward': 0.2, 'success_rate': 0.6, 'episodes': 10, 'total_reward': 2}
[20:15:39] RL Agent: Training complete. Metrics: {'avg_reward': 0.2, 'success_rate': 0.6, 'episodes': 10}
[20:15:39] RL Agent: Process complete.
[20:15:39] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[20:15:39] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[20:15:39] Orchestrator: Run complete.
[00:25:20] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[00:25:20] InputAgent: Loading case from data\inputs\mumbai_case.json
[00:25:20] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[00:25:20] Would download: https://example.com/regulations.pdf
[00:25:20] Parsing (stub): rules_kb/sample_rules.json
[00:25:20] Classification Agent: Starting to select applicable rules.
[00:25:20] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:25:20] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:25:20] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:25:20] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:25:20] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:25:20] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:25:20] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:25:20] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:25:20] Classification Agent: Found 0 applicable rules.
[00:25:20] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=None, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[00:25:20] Calculation Agent: Starting computations.
[00:25:20] Calculation Agent: Processing case with plot size: 500 sqm
[00:25:20] Calculation Agent: Location is urban
[00:25:20] Calculation Agent: No specific setback rule found, using default: 1.5
[00:25:20] Calculation Agent: No specific coverage rule found, using default: 0.55
[00:25:20] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:25:20] Calculation Agent: No additional FAR rule found
[00:25:20] Calculation Agent: Max footprint: 275.0 sqm, Total floor area: 750.0 sqm
[00:25:20] Calculation Agent: Finished. Total floor area: 750.0 sqm.
[00:25:20] RL Agent: Starting RL decision process.
[00:25:20] RL Agent: Expected rule path: []
[00:25:20] RL Agent: Candidate 1 (expected): []
[00:25:20] RL Agent: Candidate 2 (reversed): []
[00:25:20] RL Agent: Initializing RL environment and training...
[00:25:20] RL Environment: Initialized with 2 candidate paths
[00:25:20] RL Environment: Candidate 0: []
[00:25:20] RL Environment: Candidate 1: []
[00:25:20] Training RL (random policy) for 10 episodes...
[00:25:20] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:25:20] RL Environment: Reset
[00:25:20] RL Environment: Feedback reward for case mumbai-001: 0
[00:25:20] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:25:20] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:25:20] EP 1: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:25:20] RL Environment: Reset
[00:25:20] RL Environment: Feedback reward for case mumbai-001: 0
[00:25:20] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:25:20] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:25:20] EP 2: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:25:20] RL Environment: Reset
[00:25:20] RL Environment: Feedback reward for case mumbai-001: 0
[00:25:20] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:25:20] RL Training - Episode 3: {'episode': 3, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:25:20] EP 3: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:25:20] RL Environment: Reset
[00:25:20] RL Environment: Feedback reward for case mumbai-001: 0
[00:25:20] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:25:20] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:25:20] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:25:20] RL Environment: Reset
[00:25:20] RL Environment: Feedback reward for case mumbai-001: 0
[00:25:20] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:25:20] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:25:20] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:25:20] RL Environment: Reset
[00:25:20] RL Environment: Feedback reward for case mumbai-001: 0
[00:25:20] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:25:20] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:25:20] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:25:20] RL Environment: Reset
[00:25:20] RL Environment: Feedback reward for case mumbai-001: 0
[00:25:20] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:25:20] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:25:20] EP 7: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:25:20] RL Environment: Reset
[00:25:20] RL Environment: Feedback reward for case mumbai-001: 0
[00:25:20] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:25:20] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:25:20] EP 8: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:25:20] RL Environment: Reset
[00:25:20] RL Environment: Feedback reward for case mumbai-001: 0
[00:25:20] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:25:20] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:25:20] EP 9: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:25:20] RL Environment: Reset
[00:25:20] RL Environment: Feedback reward for case mumbai-001: 0
[00:25:20] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:25:20] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:25:20] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:25:20] Avg reward: 1.00, Success rate: 100.00%
[00:25:20] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[00:25:20] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[00:25:20] RL Agent: Process complete.
[00:25:20] Orchestrator: Saved JSON report to outputs\json\mumbai_output.json
[00:25:20] Orchestrator: Saved geometry to outputs\geometry\mumbai_model.stl
[00:25:20] Orchestrator: Run complete.
[00:43:15] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[00:43:15] InputAgent: Loading case from data\inputs\mumbai_case.json
[00:43:15] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[00:43:15] Would download: https://example.com/regulations.pdf
[00:43:15] Parsing (stub): rules_kb/sample_rules.json
[00:43:15] Classification Agent: Starting to select applicable rules.
[00:43:15] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:43:15] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:43:15] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:43:15] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:43:15] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:43:15] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:43:15] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:43:15] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:43:15] Classification Agent: Found 0 applicable rules.
[00:43:15] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=None, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[00:43:15] Calculation Agent: Starting computations.
[00:43:15] Calculation Agent: Processing case with plot size: 500 sqm
[00:43:15] Calculation Agent: Location is urban
[00:43:15] Calculation Agent: No specific setback rule found, using default: 1.5
[00:43:15] Calculation Agent: No specific coverage rule found, using default: 0.55
[00:43:15] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:43:15] Calculation Agent: No additional FAR rule found
[00:43:15] Calculation Agent: Max footprint: 275.0 sqm, Total floor area: 750.0 sqm
[00:43:15] Calculation Agent: Finished. Total floor area: 750.0 sqm.
[00:43:15] RL Agent: Starting RL decision process.
[00:43:15] RL Agent: Expected rule path: []
[00:43:15] RL Agent: Candidate 1 (expected): []
[00:43:15] RL Agent: Candidate 2 (reversed): []
[00:43:15] RL Agent: Initializing RL environment and training...
[00:43:15] RL Environment: Initialized with 2 candidate paths
[00:43:15] RL Environment: Candidate 0: []
[00:43:15] RL Environment: Candidate 1: []
[00:43:15] Training RL (random policy) for 10 episodes...
[00:43:15] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:43:15] RL Environment: Reset
[00:43:15] RL Environment: Feedback reward for case mumbai-001: 0
[00:43:15] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:43:15] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:43:15] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:43:15] RL Environment: Reset
[00:43:15] RL Environment: Feedback reward for case mumbai-001: 0
[00:43:15] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:43:15] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:43:15] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:43:15] RL Environment: Reset
[00:43:15] RL Environment: Feedback reward for case mumbai-001: 0
[00:43:15] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:43:15] RL Training - Episode 3: {'episode': 3, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:43:15] EP 3: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:43:15] RL Environment: Reset
[00:43:15] RL Environment: Feedback reward for case mumbai-001: 0
[00:43:15] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:43:15] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:43:15] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:43:15] RL Environment: Reset
[00:43:15] RL Environment: Feedback reward for case mumbai-001: 0
[00:43:15] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:43:15] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:43:15] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:43:15] RL Environment: Reset
[00:43:15] RL Environment: Feedback reward for case mumbai-001: 0
[00:43:15] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:43:15] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:43:15] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:43:15] RL Environment: Reset
[00:43:15] RL Environment: Feedback reward for case mumbai-001: 0
[00:43:15] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:43:15] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:43:15] EP 7: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:43:15] RL Environment: Reset
[00:43:15] RL Environment: Feedback reward for case mumbai-001: 0
[00:43:15] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:43:15] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:43:15] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:43:15] RL Environment: Reset
[00:43:15] RL Environment: Feedback reward for case mumbai-001: 0
[00:43:15] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:43:15] RL Training - Episode 9: {'episode': 9, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:43:15] EP 9: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:43:15] RL Environment: Reset
[00:43:15] RL Environment: Feedback reward for case mumbai-001: 0
[00:43:15] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:43:15] RL Training - Episode 10: {'episode': 10, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:43:15] EP 10: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:43:15] Avg reward: 1.00, Success rate: 100.00%
[00:43:15] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[00:43:15] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[00:43:15] RL Agent: Process complete.
[00:43:15] Orchestrator: Saved JSON report to outputs\json\mumbai_output.json
[00:43:15] Orchestrator: Saved geometry to outputs\geometry\mumbai_model.stl
[00:43:15] Orchestrator: Run complete.
[00:49:32] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[00:49:32] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[00:49:32] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[00:49:32] Would download: https://example.com/regulations.pdf
[00:49:32] Parsing (stub): rules_kb/sample_rules.json
[00:49:32] Classification Agent: Starting to select applicable rules.
[00:49:32] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:49:32] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[00:49:32] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[00:49:32] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[00:49:32] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[00:49:32] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:49:32] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:49:32] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:49:32] Classification Agent: Found 5 applicable rules.
[00:49:32] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[00:49:32] Calculation Agent: Starting computations.
[00:49:32] Calculation Agent: Processing case with plot size: 800 sqm
[00:49:32] Calculation Agent: Location is urban
[00:49:32] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[00:49:32] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[00:49:32] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[00:49:32] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:49:32] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[00:49:32] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[00:49:32] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[00:49:32] RL Agent: Starting RL decision process.
[00:49:32] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:49:32] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:49:32] RL Agent: Initializing RL environment and training...
[00:49:32] RL Environment: Initialized with 3 candidate paths
[00:49:32] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:49:32] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:49:32] Training RL (random policy) for 10 episodes...
[00:49:32] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:49:32] RL Environment: Reset
[00:49:32] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:49:32] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:49:32] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:49:32] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Environment: Reset
[00:49:32] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:49:32] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:49:32] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:49:32] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Environment: Reset
[00:49:32] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:49:32] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:49:32] RL Training - Episode 3: {'episode': 3, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.3333333333333333, 'success_rate': 0.6666666666666666}
[00:49:32] EP 3: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Environment: Reset
[00:49:32] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:49:32] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:49:32] RL Training - Episode 4: {'episode': 4, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[00:49:32] EP 4: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Environment: Reset
[00:49:32] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:49:32] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:49:32] RL Training - Episode 5: {'episode': 5, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.2, 'success_rate': 0.4}
[00:49:32] EP 5: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Environment: Reset
[00:49:32] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:49:32] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:49:32] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[00:49:32] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Environment: Reset
[00:49:32] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:49:32] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:49:32] RL Training - Episode 7: {'episode': 7, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.14285714285714285, 'success_rate': 0.42857142857142855}
[00:49:32] EP 7: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Environment: Reset
[00:49:32] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:49:32] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:49:32] RL Training - Episode 8: {'episode': 8, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.25, 'success_rate': 0.375}
[00:49:32] EP 8: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Environment: Reset
[00:49:32] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:49:32] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:49:32] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[00:49:32] EP 9: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] RL Environment: Reset
[00:49:32] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:49:32] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:49:32] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.2, 'success_rate': 0.4}
[00:49:32] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:49:32] Avg reward: -0.20, Success rate: 40.00%
[00:49:32] RL Training Summary: {'status': 'training_completed', 'avg_reward': -0.2, 'success_rate': 0.4, 'episodes': 10, 'total_reward': -2}
[00:49:32] RL Agent: Training complete. Metrics: {'avg_reward': -0.2, 'success_rate': 0.4, 'episodes': 10}
[00:49:32] RL Agent: Process complete.
[00:49:32] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[00:49:32] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[00:49:32] Orchestrator: Run complete.
[00:51:19] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[00:51:19] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[00:51:19] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[00:51:19] Would download: https://example.com/regulations.pdf
[00:51:19] Parsing (stub): rules_kb/sample_rules.json
[00:51:19] Classification Agent: Starting to select applicable rules.
[00:51:19] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:51:19] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[00:51:19] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[00:51:19] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[00:51:19] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[00:51:19] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:51:19] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:51:19] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:51:19] Classification Agent: Found 5 applicable rules.
[00:51:19] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[00:51:19] Calculation Agent: Starting computations.
[00:51:19] Calculation Agent: Processing case with plot size: 800 sqm
[00:51:19] Calculation Agent: Location is urban
[00:51:19] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[00:51:19] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[00:51:19] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[00:51:19] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:51:19] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[00:51:19] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[00:51:19] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[00:51:19] RL Agent: Starting RL decision process.
[00:51:19] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:51:19] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:51:19] RL Agent: Initializing RL environment and training...
[00:51:19] RL Environment: Initialized with 3 candidate paths
[00:51:19] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:51:19] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:51:19] Training RL (random policy) for 10 episodes...
[00:51:19] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:51:19] RL Environment: Reset
[00:51:19] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:51:19] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:51:19] RL Training - Episode 1: {'episode': 1, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:51:19] EP 1: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Environment: Reset
[00:51:19] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:51:19] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:51:19] RL Training - Episode 2: {'episode': 2, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:51:19] EP 2: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Environment: Reset
[00:51:19] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:51:19] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:51:19] RL Training - Episode 3: {'episode': 3, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:51:19] EP 3: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Environment: Reset
[00:51:19] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:51:19] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:51:19] RL Training - Episode 4: {'episode': 4, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:51:19] EP 4: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Environment: Reset
[00:51:19] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:51:19] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:51:19] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:51:19] EP 5: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Environment: Reset
[00:51:19] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:51:19] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:51:19] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:51:19] EP 6: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Environment: Reset
[00:51:19] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:51:19] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:19] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.7142857142857143, 'success_rate': 0.14285714285714285}
[00:51:19] EP 7: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Environment: Reset
[00:51:19] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:51:19] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:51:19] RL Training - Episode 8: {'episode': 8, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.75, 'success_rate': 0.125}
[00:51:19] EP 8: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Environment: Reset
[00:51:19] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:51:19] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:19] RL Training - Episode 9: {'episode': 9, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.5555555555555556, 'success_rate': 0.2222222222222222}
[00:51:19] EP 9: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] RL Environment: Reset
[00:51:19] RL Environment: Feedback reward for case mumbai-redev-001: 0
[00:51:19] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:51:19] RL Training - Episode 10: {'episode': 10, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.6, 'success_rate': 0.2}
[00:51:19] EP 10: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:51:19] Avg reward: -0.60, Success rate: 20.00%
[00:51:19] RL Training Summary: {'status': 'training_completed', 'avg_reward': -0.6, 'success_rate': 0.2, 'episodes': 10, 'total_reward': -6}
[00:51:19] RL Agent: Training complete. Metrics: {'avg_reward': -0.6, 'success_rate': 0.2, 'episodes': 10}
[00:51:19] RL Agent: Process complete.
[00:51:19] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[00:51:19] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[00:51:19] Orchestrator: Run complete.
[00:51:56] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[00:51:56] InputAgent: Loading case from data\inputs\mumbai_case.json
[00:51:56] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[00:51:56] Would download: https://example.com/regulations.pdf
[00:51:56] Parsing (stub): rules_kb/sample_rules.json
[00:51:56] Classification Agent: Starting to select applicable rules.
[00:51:56] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:51:56] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:51:56] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:51:56] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:51:56] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:51:56] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:51:56] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:51:56] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:51:56] Classification Agent: Found 0 applicable rules.
[00:51:56] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=None, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[00:51:56] Calculation Agent: Starting computations.
[00:51:56] Calculation Agent: Processing case with plot size: 500 sqm
[00:51:56] Calculation Agent: Location is urban
[00:51:56] Calculation Agent: No specific setback rule found, using default: 1.5
[00:51:56] Calculation Agent: No specific coverage rule found, using default: 0.55
[00:51:56] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:51:56] Calculation Agent: No additional FAR rule found
[00:51:56] Calculation Agent: Max footprint: 275.0 sqm, Total floor area: 750.0 sqm
[00:51:56] Calculation Agent: Finished. Total floor area: 750.0 sqm.
[00:51:56] RL Agent: Starting RL decision process.
[00:51:56] RL Agent: Expected rule path: []
[00:51:56] RL Agent: Candidate 1 (expected): []
[00:51:56] RL Agent: Candidate 2 (reversed): []
[00:51:56] RL Agent: Initializing RL environment and training...
[00:51:56] RL Environment: Initialized with 2 candidate paths
[00:51:56] RL Environment: Candidate 0: []
[00:51:56] RL Environment: Candidate 1: []
[00:51:56] Training RL (random policy) for 10 episodes...
[00:51:56] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 1: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 2: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 3: {'episode': 3, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 3: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 5: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 7: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 8: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 9: {'episode': 9, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 9: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] Avg reward: 1.00, Success rate: 100.00%
[00:51:56] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[00:51:56] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[00:51:56] RL Agent: Process complete.
[00:51:56] Orchestrator: Saved JSON report to outputs\json\mumbai_output.json
[00:51:56] Orchestrator: Saved geometry to outputs\geometry\mumbai_model.stl
[00:51:56] Orchestrator: Run complete.
[00:51:56] Orchestrator: Starting run for case: data/inputs/pune_case.json
[00:51:56] InputAgent: Loading case from data\inputs\pune_case.json
[00:51:56] InputAgent: Successfully loaded case with keys: ['city', 'type', 'plot_size']
[00:51:56] Would download: https://example.com/regulations.pdf
[00:51:56] Parsing (stub): rules_kb/sample_rules.json
[00:51:56] Classification Agent: Starting to select applicable rules.
[00:51:56] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:51:56] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[00:51:56] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[00:51:56] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[00:51:56] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[00:51:56] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:51:56] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:51:56] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:51:56] Classification Agent: Found 0 applicable rules.
[00:51:56] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=Pune, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[00:51:56] Calculation Agent: Starting computations.
[00:51:56] Calculation Agent: Processing case with plot size: 400 sqm
[00:51:56] Calculation Agent: Location is default
[00:51:56] Calculation Agent: No specific setback rule found, using default: 1.5
[00:51:56] Calculation Agent: No specific coverage rule found, using default: 0.55
[00:51:56] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:51:56] Calculation Agent: No additional FAR rule found
[00:51:56] Calculation Agent: Max footprint: 220.00000000000003 sqm, Total floor area: 600.0 sqm
[00:51:56] Calculation Agent: Finished. Total floor area: 600.0 sqm.
[00:51:56] RL Agent: Starting RL decision process.
[00:51:56] RL Agent: Expected rule path: []
[00:51:56] RL Agent: Candidate 1 (expected): []
[00:51:56] RL Agent: Candidate 2 (reversed): []
[00:51:56] RL Agent: Initializing RL environment and training...
[00:51:56] RL Environment: Initialized with 2 candidate paths
[00:51:56] RL Environment: Candidate 0: []
[00:51:56] RL Environment: Candidate 1: []
[00:51:56] Training RL (random policy) for 10 episodes...
[00:51:56] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 1: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 2: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 3: {'episode': 3, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 3: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 5: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 7: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 8: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 9: {'episode': 9, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 9: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] RL Environment: Reset
[00:51:56] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:51:56] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[00:51:56] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[00:51:56] Avg reward: 1.00, Success rate: 100.00%
[00:51:56] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[00:51:56] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[00:51:56] RL Agent: Process complete.
[00:51:56] Orchestrator: Saved JSON report to outputs\json\pune_output.json
[00:51:56] Orchestrator: Saved geometry to outputs\geometry\pune_model.stl
[00:51:56] Orchestrator: Run complete.
[00:55:44] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[00:55:44] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[00:55:44] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[00:55:44] Would download: https://example.com/regulations.pdf
[00:55:44] Parsing (stub): rules_kb/sample_rules.json
[00:55:44] Classification Agent: Starting to select applicable rules.
[00:55:44] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[00:55:44] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[00:55:44] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[00:55:44] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[00:55:44] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[00:55:44] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[00:55:44] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[00:55:44] Classification Agent: REJECTED rule AHM-DCR-FAR
[00:55:44] Classification Agent: Found 5 applicable rules.
[00:55:44] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[00:55:44] Calculation Agent: Starting computations.
[00:55:44] Calculation Agent: Processing case with plot size: 800 sqm
[00:55:44] Calculation Agent: Location is urban
[00:55:44] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[00:55:44] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[00:55:44] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[00:55:44] Calculation Agent: No specific FAR rule found, using default: 1.5
[00:55:44] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[00:55:44] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[00:55:44] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[00:55:44] RL Agent: Starting RL decision process.
[00:55:44] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:55:44] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:55:44] RL Agent: Initializing RL environment and training...
[00:55:44] RL Environment: Initialized with 3 candidate paths
[00:55:44] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:55:44] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[00:55:44] Training RL (random policy) for 10 episodes...
[00:55:44] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[00:55:44] RL Environment: Reset
[00:55:44] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:55:44] RL Training - Episode 1: {'episode': 1, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:55:44] EP 1: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Environment: Reset
[00:55:44] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:55:44] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:55:44] EP 2: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Environment: Reset
[00:55:44] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:55:44] RL Training - Episode 3: {'episode': 3, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:55:44] EP 3: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Environment: Reset
[00:55:44] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:55:44] RL Training - Episode 4: {'episode': 4, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[00:55:44] EP 4: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Environment: Reset
[00:55:44] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:55:44] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.6, 'success_rate': 0.2}
[00:55:44] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Environment: Reset
[00:55:44] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:55:44] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.6666666666666666, 'success_rate': 0.16666666666666666}
[00:55:44] EP 6: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Environment: Reset
[00:55:44] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:55:44] RL Training - Episode 7: {'episode': 7, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.7142857142857143, 'success_rate': 0.14285714285714285}
[00:55:44] EP 7: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Environment: Reset
[00:55:44] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[00:55:44] RL Training - Episode 8: {'episode': 8, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.75, 'success_rate': 0.125}
[00:55:44] EP 8: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Environment: Reset
[00:55:44] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:55:44] RL Training - Episode 9: {'episode': 9, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.5555555555555556, 'success_rate': 0.2222222222222222}
[00:55:44] EP 9: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] RL Environment: Reset
[00:55:44] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[00:55:44] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.4, 'success_rate': 0.3}
[00:55:44] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[00:55:44] Avg reward: -0.40, Success rate: 30.00%
[00:55:44] RL Training Summary: {'status': 'training_completed', 'avg_reward': -0.4, 'success_rate': 0.3, 'episodes': 10, 'total_reward': -4}
[00:55:44] RL Agent: Training complete. Metrics: {'avg_reward': -0.4, 'success_rate': 0.3, 'episodes': 10}
[00:55:44] RL Agent: Process complete.
[00:55:44] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[00:55:44] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[00:55:44] Orchestrator: Run complete.
[01:00:11] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[01:00:11] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[01:00:11] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[01:00:11] Would download: https://example.com/regulations.pdf
[01:00:11] Parsing (stub): rules_kb/sample_rules.json
[01:00:11] Classification Agent: Starting to select applicable rules.
[01:00:11] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[01:00:11] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[01:00:11] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[01:00:11] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[01:00:11] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[01:00:11] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[01:00:11] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[01:00:11] Classification Agent: REJECTED rule AHM-DCR-FAR
[01:00:11] Classification Agent: Found 5 applicable rules.
[01:00:11] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[01:00:11] Calculation Agent: Starting computations.
[01:00:11] Calculation Agent: Processing case with plot size: 800 sqm
[01:00:11] Calculation Agent: Location is urban
[01:00:11] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[01:00:11] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[01:00:11] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[01:00:11] Calculation Agent: No specific FAR rule found, using default: 1.5
[01:00:11] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[01:00:11] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[01:00:11] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[01:00:11] RL Agent: Starting RL decision process.
[01:00:11] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[01:00:11] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[01:00:11] RL Agent: Initializing RL environment and training...
[01:00:11] RL Environment: Initialized with 3 candidate paths
[01:00:11] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[01:00:11] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[01:00:11] Training RL (random policy) for 10 episodes...
[01:00:11] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[01:00:11] RL Environment: Reset
[01:00:11] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:11] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:11] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Environment: Reset
[01:00:11] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:11] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:11] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Environment: Reset
[01:00:11] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[01:00:11] RL Training - Episode 3: {'episode': 3, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.3333333333333333, 'success_rate': 0.6666666666666666}
[01:00:11] EP 3: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Environment: Reset
[01:00:11] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:11] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.5, 'success_rate': 0.75}
[01:00:11] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Environment: Reset
[01:00:11] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[01:00:11] RL Training - Episode 5: {'episode': 5, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.2, 'success_rate': 0.6}
[01:00:11] EP 5: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Environment: Reset
[01:00:11] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[01:00:11] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[01:00:11] EP 6: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Environment: Reset
[01:00:11] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[01:00:11] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.14285714285714285, 'success_rate': 0.42857142857142855}
[01:00:11] EP 7: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Environment: Reset
[01:00:11] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[01:00:11] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.25, 'success_rate': 0.375}
[01:00:11] EP 8: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Environment: Reset
[01:00:11] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[01:00:11] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[01:00:11] EP 9: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] RL Environment: Reset
[01:00:11] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[01:00:11] RL Training - Episode 10: {'episode': 10, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -0.4, 'success_rate': 0.3}
[01:00:11] EP 10: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[01:00:11] Avg reward: -0.40, Success rate: 30.00%
[01:00:11] RL Training Summary: {'status': 'training_completed', 'avg_reward': -0.4, 'success_rate': 0.3, 'episodes': 10, 'total_reward': -4}
[01:00:11] RL Agent: Training complete. Metrics: {'avg_reward': -0.4, 'success_rate': 0.3, 'episodes': 10}
[01:00:11] RL Agent: Process complete.
[01:00:11] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[01:00:11] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[01:00:11] Orchestrator: Run complete.
[01:00:34] Feedback received for case test-case-001: up
[01:00:48] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[01:00:48] InputAgent: Loading case from data\inputs\mumbai_case.json
[01:00:48] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[01:00:48] Would download: https://example.com/regulations.pdf
[01:00:48] Parsing (stub): rules_kb/sample_rules.json
[01:00:48] Classification Agent: Starting to select applicable rules.
[01:00:48] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[01:00:48] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[01:00:48] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[01:00:48] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[01:00:48] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[01:00:48] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[01:00:48] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[01:00:48] Classification Agent: REJECTED rule AHM-DCR-FAR
[01:00:48] Classification Agent: Found 0 applicable rules.
[01:00:48] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=None, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[01:00:48] Calculation Agent: Starting computations.
[01:00:48] Calculation Agent: Processing case with plot size: 500 sqm
[01:00:48] Calculation Agent: Location is urban
[01:00:48] Calculation Agent: No specific setback rule found, using default: 1.5
[01:00:48] Calculation Agent: No specific coverage rule found, using default: 0.55
[01:00:48] Calculation Agent: No specific FAR rule found, using default: 1.5
[01:00:48] Calculation Agent: No additional FAR rule found
[01:00:48] Calculation Agent: Max footprint: 275.0 sqm, Total floor area: 750.0 sqm
[01:00:48] Calculation Agent: Finished. Total floor area: 750.0 sqm.
[01:00:48] RL Agent: Starting RL decision process.
[01:00:48] RL Agent: Expected rule path: []
[01:00:48] RL Agent: Candidate 1 (expected): []
[01:00:48] RL Agent: Candidate 2 (reversed): []
[01:00:48] RL Agent: Initializing RL environment and training...
[01:00:48] RL Environment: Initialized with 2 candidate paths
[01:00:48] RL Environment: Candidate 0: []
[01:00:48] RL Environment: Candidate 1: []
[01:00:48] Training RL (random policy) for 10 episodes...
[01:00:48] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[01:00:48] RL Environment: Reset
[01:00:48] RL Environment: Feedback reward for case mumbai-001: 0
[01:00:48] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:48] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:48] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[01:00:48] RL Environment: Reset
[01:00:48] RL Environment: Feedback reward for case mumbai-001: 0
[01:00:48] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:48] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:48] EP 2: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[01:00:48] RL Environment: Reset
[01:00:48] RL Environment: Feedback reward for case mumbai-001: 0
[01:00:48] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:48] RL Training - Episode 3: {'episode': 3, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:48] EP 3: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[01:00:48] RL Environment: Reset
[01:00:48] RL Environment: Feedback reward for case mumbai-001: 0
[01:00:48] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:48] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:48] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[01:00:48] RL Environment: Reset
[01:00:48] RL Environment: Feedback reward for case mumbai-001: 0
[01:00:48] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:48] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:48] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[01:00:48] RL Environment: Reset
[01:00:48] RL Environment: Feedback reward for case mumbai-001: 0
[01:00:48] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:48] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:48] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[01:00:48] RL Environment: Reset
[01:00:48] RL Environment: Feedback reward for case mumbai-001: 0
[01:00:48] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:48] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:48] EP 7: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[01:00:48] RL Environment: Reset
[01:00:48] RL Environment: Feedback reward for case mumbai-001: 0
[01:00:48] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:48] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:48] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[01:00:48] RL Environment: Reset
[01:00:48] RL Environment: Feedback reward for case mumbai-001: 0
[01:00:48] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:48] RL Training - Episode 9: {'episode': 9, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:48] EP 9: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[01:00:48] RL Environment: Reset
[01:00:48] RL Environment: Feedback reward for case mumbai-001: 0
[01:00:48] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[01:00:48] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[01:00:48] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[01:00:48] Avg reward: 1.00, Success rate: 100.00%
[01:00:48] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[01:00:48] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[01:00:48] RL Agent: Process complete.
[01:00:48] Orchestrator: Saved JSON report to outputs\json\mumbai_output.json
[01:00:48] Orchestrator: Saved geometry to outputs\geometry\mumbai_model.stl
[01:00:48] Orchestrator: Run complete.
[01:08:45] Feedback received for case test-case-001: up
[01:14:46] Feedback received for case test-case-001: up
[01:19:41] Feedback received for case test-down-manual: down
[01:27:35] Feedback received for case test-down-manual: down
[21:14:45] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[21:14:45] InputAgent: Loading case from data\inputs\mumbai_case.json
[21:14:45] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[21:14:45] Would download: https://example.com/regulations.pdf
[21:14:45] Parsing (stub): rules_kb/sample_rules.json
[21:14:45] Classification Agent: Starting to select applicable rules.
[21:14:45] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[21:14:45] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[21:14:45] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[21:14:45] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[21:14:45] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[21:14:45] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[21:14:45] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[21:14:45] Classification Agent: REJECTED rule AHM-DCR-FAR
[21:14:45] Classification Agent: Found 0 applicable rules.
[21:14:45] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=None, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[21:14:45] Calculation Agent: Starting computations.
[21:14:45] Calculation Agent: Processing case with plot size: 500 sqm
[21:14:45] Calculation Agent: Location is urban
[21:14:45] Calculation Agent: No specific setback rule found, using default: 1.5
[21:14:45] Calculation Agent: No specific coverage rule found, using default: 0.55
[21:14:45] Calculation Agent: No specific FAR rule found, using default: 1.5
[21:14:45] Calculation Agent: No additional FAR rule found
[21:14:45] Calculation Agent: Max footprint: 275.0 sqm, Total floor area: 750.0 sqm
[21:14:45] Calculation Agent: Finished. Total floor area: 750.0 sqm.
[21:14:45] RL Agent: Starting RL decision process.
[21:14:45] RL Agent: Expected rule path: []
[21:14:45] RL Agent: Candidate 1 (expected): []
[21:14:45] RL Agent: Candidate 2 (reversed): []
[21:14:45] RL Agent: Initializing RL environment and training...
[21:14:45] RL Environment: Initialized with 2 candidate paths
[21:14:45] RL Environment: Candidate 0: []
[21:14:45] RL Environment: Candidate 1: []
[21:14:45] Training RL (random policy) for 10 episodes...
[21:14:45] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[21:14:45] RL Environment: Reset
[21:14:45] RL Environment: Feedback reward for case mumbai-001: 0
[21:14:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:14:45] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:14:45] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:14:45] RL Environment: Reset
[21:14:45] RL Environment: Feedback reward for case mumbai-001: 0
[21:14:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:14:45] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:14:45] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:14:45] RL Environment: Reset
[21:14:45] RL Environment: Feedback reward for case mumbai-001: 0
[21:14:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:14:45] RL Training - Episode 3: {'episode': 3, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:14:45] EP 3: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:14:45] RL Environment: Reset
[21:14:45] RL Environment: Feedback reward for case mumbai-001: 0
[21:14:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:14:45] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:14:45] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:14:45] RL Environment: Reset
[21:14:45] RL Environment: Feedback reward for case mumbai-001: 0
[21:14:45] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:14:45] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:14:45] EP 5: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:14:45] RL Environment: Reset
[21:14:45] RL Environment: Feedback reward for case mumbai-001: 0
[21:14:45] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:14:45] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:14:45] EP 6: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:14:45] RL Environment: Reset
[21:14:45] RL Environment: Feedback reward for case mumbai-001: 0
[21:14:45] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:14:45] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:14:45] EP 7: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:14:45] RL Environment: Reset
[21:14:45] RL Environment: Feedback reward for case mumbai-001: 0
[21:14:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:14:45] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:14:45] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:14:45] RL Environment: Reset
[21:14:45] RL Environment: Feedback reward for case mumbai-001: 0
[21:14:45] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:14:45] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:14:45] EP 9: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:14:45] RL Environment: Reset
[21:14:45] RL Environment: Feedback reward for case mumbai-001: 0
[21:14:45] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:14:45] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:14:45] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:14:45] Avg reward: 1.00, Success rate: 100.00%
[21:14:45] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[21:14:45] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[21:14:45] RL Agent: Process complete.
[21:14:45] Orchestrator: Saved JSON report to outputs\json\mumbai_output.json
[21:14:45] Orchestrator: Saved geometry to outputs\geometry\mumbai_model.stl
[21:14:45] Orchestrator: Run complete.
[21:17:02] Orchestrator: Starting run for case: data/inputs/mumbai_redevelopment.json
[21:17:02] InputAgent: Loading case from data\inputs\mumbai_redevelopment.json
[21:17:02] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'project_type', 'road_width', 'plot_size', 'expected_rule_path']
[21:17:02] Would download: https://example.com/regulations.pdf
[21:17:02] Parsing (stub): rules_kb/sample_rules.json
[21:17:02] Classification Agent: Starting to select applicable rules.
[21:17:02] Classification Agent: SELECTED rule MUM-DCPR-2034-SETBACK-FRONT
[21:17:02] Classification Agent: SELECTED rule MUM-DCPR-2034-COVERAGE
[21:17:02] Classification Agent: SELECTED rule MUM-DCPR-2034-FAR
[21:17:02] Classification Agent: SELECTED rule MUM-MCGM-HEIGHT
[21:17:02] Classification Agent: SELECTED rule MUM-MHADA-REDEVELOPMENT
[21:17:02] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[21:17:02] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[21:17:02] Classification Agent: REJECTED rule AHM-DCR-FAR
[21:17:02] Classification Agent: Found 5 applicable rules.
[21:17:02] Classification Agent: Decision details - SELECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-COVERAGE: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-DCPR-2034-FAR: Condition match: city=Mumbai, Condition match: plot_type=residential, SELECTED rule MUM-MCGM-HEIGHT: Condition match: city=Mumbai, SELECTED rule MUM-MHADA-REDEVELOPMENT: Condition match: city=Mumbai, Condition match: project_type=slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Mumbai, expected Ahmedabad, Condition match: plot_type=residential
[21:17:02] Calculation Agent: Starting computations.
[21:17:02] Calculation Agent: Processing case with plot size: 800 sqm
[21:17:02] Calculation Agent: Location is urban
[21:17:02] Calculation Agent: Evaluating setback for rule MUM-DCPR-2034-SETBACK-FRONT with road width 15
[21:17:02] Calculation Agent: Applied setback rule: road_width >= 12 -> setback = 3.0
[21:17:02] Calculation Agent: Coverage for urban is 0.65 (from rule MUM-DCPR-2034-COVERAGE)
[21:17:02] Calculation Agent: No specific FAR rule found, using default: 1.5
[21:17:02] Calculation Agent: Additional FAR from MUM-MHADA-REDEVELOPMENT is 1.0
[21:17:02] Calculation Agent: Max footprint: 520.0 sqm, Total floor area: 2000.0 sqm
[21:17:02] Calculation Agent: Finished. Total floor area: 2000.0 sqm.
[21:17:02] RL Agent: Starting RL decision process.
[21:17:02] RL Agent: Expected rule path: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Agent: Candidate 1 (expected): ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Agent: Candidate 2 (reversed): ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[21:17:02] RL Agent: Candidate 3 (rotated): ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[21:17:02] RL Agent: Initializing RL environment and training...
[21:17:02] RL Environment: Initialized with 3 candidate paths
[21:17:02] RL Environment: Candidate 0: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Environment: Candidate 1: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT']
[21:17:02] RL Environment: Candidate 2: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT']
[21:17:02] Training RL (random policy) for 10 episodes...
[21:17:02] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[21:17:02] RL Environment: Reset
[21:17:02] RL Environment: Feedback reward for case mumbai-redev-001: 0
[21:17:02] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:02] RL Training - Episode 1: {'episode': 1, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': -1.0, 'success_rate': 0.0}
[21:17:02] EP 1: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Environment: Reset
[21:17:02] RL Environment: Feedback reward for case mumbai-redev-001: 0
[21:17:02] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:02] RL Training - Episode 2: {'episode': 2, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[21:17:02] EP 2: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Environment: Reset
[21:17:02] RL Environment: Feedback reward for case mumbai-redev-001: 0
[21:17:02] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:02] RL Training - Episode 3: {'episode': 3, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.3333333333333333, 'success_rate': 0.6666666666666666}
[21:17:02] EP 3: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Environment: Reset
[21:17:02] RL Environment: Feedback reward for case mumbai-redev-001: 0
[21:17:02] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:02] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.5, 'success_rate': 0.75}
[21:17:02] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Environment: Reset
[21:17:02] RL Environment: Feedback reward for case mumbai-redev-001: 0
[21:17:02] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:02] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.6, 'success_rate': 0.8}
[21:17:02] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Environment: Reset
[21:17:02] RL Environment: Feedback reward for case mumbai-redev-001: 0
[21:17:02] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:02] RL Training - Episode 6: {'episode': 6, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.3333333333333333, 'success_rate': 0.6666666666666666}
[21:17:02] EP 6: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Environment: Reset
[21:17:02] RL Environment: Feedback reward for case mumbai-redev-001: 0
[21:17:02] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:02] RL Training - Episode 7: {'episode': 7, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.42857142857142855, 'success_rate': 0.7142857142857143}
[21:17:02] EP 7: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Environment: Reset
[21:17:02] RL Environment: Feedback reward for case mumbai-redev-001: 0
[21:17:02] RL Environment: Action 1 -> Chosen: ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:02] RL Training - Episode 8: {'episode': 8, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.25, 'success_rate': 0.625}
[21:17:02] EP 8: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-FAR', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Environment: Reset
[21:17:02] RL Environment: Feedback reward for case mumbai-redev-001: 0
[21:17:02] RL Environment: Action 2 -> Chosen: ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:02] RL Training - Episode 9: {'episode': 9, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.1111111111111111, 'success_rate': 0.5555555555555556}
[21:17:02] EP 9: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT', 'MUM-DCPR-2034-SETBACK-FRONT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] RL Environment: Reset
[21:17:02] RL Environment: Feedback reward for case mumbai-redev-001: 0
[21:17:02] RL Environment: Action 0 -> Chosen: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Expected: ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:02] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'expected_path': ['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'], 'running_avg_reward': 0.2, 'success_rate': 0.6}
[21:17:02] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT'] expected=['MUM-DCPR-2034-SETBACK-FRONT', 'MUM-DCPR-2034-COVERAGE', 'MUM-DCPR-2034-FAR', 'MUM-MHADA-REDEVELOPMENT']
[21:17:02] Avg reward: 0.20, Success rate: 60.00%
[21:17:02] RL Training Summary: {'status': 'training_completed', 'avg_reward': 0.2, 'success_rate': 0.6, 'episodes': 10, 'total_reward': 2}
[21:17:02] RL Agent: Training complete. Metrics: {'avg_reward': 0.2, 'success_rate': 0.6, 'episodes': 10}
[21:17:02] RL Agent: Process complete.
[21:17:02] Orchestrator: Saved JSON report to outputs\json\mumbai_redevelopment.json
[21:17:02] Orchestrator: Saved geometry to outputs\geometry\mumbai_redevelopment.json
[21:17:02] Orchestrator: Run complete.
[21:17:03] Orchestrator: Starting run for case: data/inputs/pune_case.json
[21:17:03] InputAgent: Loading case from data\inputs\pune_case.json
[21:17:03] InputAgent: Successfully loaded case with keys: ['city', 'type', 'plot_size']
[21:17:03] Would download: https://example.com/regulations.pdf
[21:17:03] Parsing (stub): rules_kb/sample_rules.json
[21:17:03] Classification Agent: Starting to select applicable rules.
[21:17:03] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[21:17:03] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[21:17:03] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[21:17:03] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[21:17:03] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[21:17:03] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[21:17:03] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[21:17:03] Classification Agent: REJECTED rule AHM-DCR-FAR
[21:17:03] Classification Agent: Found 0 applicable rules.
[21:17:03] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=Pune, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=Pune, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=Pune, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[21:17:03] Calculation Agent: Starting computations.
[21:17:03] Calculation Agent: Processing case with plot size: 400 sqm
[21:17:03] Calculation Agent: Location is default
[21:17:03] Calculation Agent: No specific setback rule found, using default: 1.5
[21:17:03] Calculation Agent: No specific coverage rule found, using default: 0.55
[21:17:03] Calculation Agent: No specific FAR rule found, using default: 1.5
[21:17:03] Calculation Agent: No additional FAR rule found
[21:17:03] Calculation Agent: Max footprint: 220.00000000000003 sqm, Total floor area: 600.0 sqm
[21:17:03] Calculation Agent: Finished. Total floor area: 600.0 sqm.
[21:17:03] RL Agent: Starting RL decision process.
[21:17:03] RL Agent: Expected rule path: []
[21:17:03] RL Agent: Candidate 1 (expected): []
[21:17:03] RL Agent: Candidate 2 (reversed): []
[21:17:03] RL Agent: Initializing RL environment and training...
[21:17:03] RL Environment: Initialized with 2 candidate paths
[21:17:03] RL Environment: Candidate 0: []
[21:17:03] RL Environment: Candidate 1: []
[21:17:03] Training RL (random policy) for 10 episodes...
[21:17:03] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case unknown: 0
[21:17:03] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case unknown: 0
[21:17:03] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 2: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case unknown: 0
[21:17:03] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 3: {'episode': 3, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 3: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case unknown: 0
[21:17:03] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 4: {'episode': 4, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 4: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case unknown: 0
[21:17:03] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case unknown: 0
[21:17:03] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case unknown: 0
[21:17:03] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 7: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case unknown: 0
[21:17:03] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case unknown: 0
[21:17:03] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 9: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case unknown: 0
[21:17:03] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 10: {'episode': 10, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 10: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[21:17:03] Avg reward: 1.00, Success rate: 100.00%
[21:17:03] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[21:17:03] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[21:17:03] RL Agent: Process complete.
[21:17:03] Orchestrator: Saved JSON report to outputs\json\pune_output.json
[21:17:03] Orchestrator: Saved geometry to outputs\geometry\pune_model.stl
[21:17:03] Orchestrator: Run complete.
[21:17:03] Orchestrator: Starting run for case: data/inputs/ahmedabad_residential.json
[21:17:03] InputAgent: Loading case from data\inputs\ahmedabad_residential.json
[21:17:03] InputAgent: Successfully loaded case with keys: ['case_id', 'city', 'location', 'plot_type', 'road_width', 'plot_size', 'expected_rule_path']
[21:17:03] Would download: https://example.com/regulations.pdf
[21:17:03] Parsing (stub): rules_kb/sample_rules.json
[21:17:03] Classification Agent: Starting to select applicable rules.
[21:17:03] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[21:17:03] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[21:17:03] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[21:17:03] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[21:17:03] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[21:17:03] Classification Agent: SELECTED rule AHM-DCR-SETBACK
[21:17:03] Classification Agent: SELECTED rule AHM-DCR-COVERAGE
[21:17:03] Classification Agent: SELECTED rule AHM-DCR-FAR
[21:17:03] Classification Agent: Found 3 applicable rules.
[21:17:03] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition match: plot_type=residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=Ahmedabad, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=Ahmedabad, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, SELECTED rule AHM-DCR-SETBACK: Condition match: city=Ahmedabad, Condition match: plot_type=residential, SELECTED rule AHM-DCR-COVERAGE: Condition match: city=Ahmedabad, Condition match: plot_type=residential, SELECTED rule AHM-DCR-FAR: Condition match: city=Ahmedabad, Condition match: plot_type=residential
[21:17:03] Calculation Agent: Starting computations.
[21:17:03] Calculation Agent: Processing case with plot size: 600 sqm
[21:17:03] Calculation Agent: Location is urban
[21:17:03] Calculation Agent: Evaluating setback for rule AHM-DCR-SETBACK with road width 12
[21:17:03] Calculation Agent: Applied setback rule: road_width >= 10 -> setback = 3.0
[21:17:03] Calculation Agent: Coverage for urban is 0.5 (from rule AHM-DCR-COVERAGE)
[21:17:03] Calculation Agent: FAR for urban is 2.0 (from rule AHM-DCR-FAR)
[21:17:03] Calculation Agent: No additional FAR rule found
[21:17:03] Calculation Agent: Max footprint: 300.0 sqm, Total floor area: 1200.0 sqm
[21:17:03] Calculation Agent: Finished. Total floor area: 1200.0 sqm.
[21:17:03] RL Agent: Starting RL decision process.
[21:17:03] RL Agent: Expected rule path: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Agent: Candidate 1 (expected): ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Agent: Candidate 2 (reversed): ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK']
[21:17:03] RL Agent: Candidate 3 (rotated): ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK']
[21:17:03] RL Agent: Initializing RL environment and training...
[21:17:03] RL Environment: Initialized with 3 candidate paths
[21:17:03] RL Environment: Candidate 0: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Environment: Candidate 1: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK']
[21:17:03] RL Environment: Candidate 2: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK']
[21:17:03] Training RL (random policy) for 10 episodes...
[21:17:03] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[21:17:03] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 1: {'episode': 1, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[21:17:03] EP 1: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[21:17:03] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:03] RL Training - Episode 2: {'episode': 2, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': 0.0, 'success_rate': 0.5}
[21:17:03] EP 2: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[21:17:03] RL Environment: Action 1 -> Chosen: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:03] RL Training - Episode 3: {'episode': 3, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.3333333333333333, 'success_rate': 0.3333333333333333}
[21:17:03] EP 3: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[21:17:03] RL Environment: Action 1 -> Chosen: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:03] RL Training - Episode 4: {'episode': 4, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.5, 'success_rate': 0.25}
[21:17:03] EP 4: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[21:17:03] RL Environment: Action 1 -> Chosen: ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:03] RL Training - Episode 5: {'episode': 5, 'action': 1, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.6, 'success_rate': 0.2}
[21:17:03] EP 5: action=1, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-FAR', 'AHM-DCR-COVERAGE', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[21:17:03] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:03] RL Training - Episode 6: {'episode': 6, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.6666666666666666, 'success_rate': 0.16666666666666666}
[21:17:03] EP 6: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[21:17:03] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:03] RL Training - Episode 7: {'episode': 7, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.7142857142857143, 'success_rate': 0.14285714285714285}
[21:17:03] EP 7: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[21:17:03] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.5, 'success_rate': 0.25}
[21:17:03] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[21:17:03] RL Environment: Action 2 -> Chosen: ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: -1, Feedback Reward: 0, Total Reward: -1
[21:17:03] RL Training - Episode 9: {'episode': 9, 'action': 2, 'base_reward': -1, 'feedback_reward': 0, 'total_reward': -1, 'chosen_path': ['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.5555555555555556, 'success_rate': 0.2222222222222222}
[21:17:03] EP 9: action=2, base_reward=-1, feedback_reward=0, total_reward=-1, chosen=['AHM-DCR-COVERAGE', 'AHM-DCR-FAR', 'AHM-DCR-SETBACK'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] RL Environment: Reset
[21:17:03] RL Environment: Feedback reward for case ahmedabad-res-001: 0
[21:17:03] RL Environment: Action 0 -> Chosen: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Expected: ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[21:17:03] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'expected_path': ['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'], 'running_avg_reward': -0.4, 'success_rate': 0.3}
[21:17:03] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR'] expected=['AHM-DCR-SETBACK', 'AHM-DCR-COVERAGE', 'AHM-DCR-FAR']
[21:17:03] Avg reward: -0.40, Success rate: 30.00%
[21:17:03] RL Training Summary: {'status': 'training_completed', 'avg_reward': -0.4, 'success_rate': 0.3, 'episodes': 10, 'total_reward': -4}
[21:17:03] RL Agent: Training complete. Metrics: {'avg_reward': -0.4, 'success_rate': 0.3, 'episodes': 10}
[21:17:03] RL Agent: Process complete.
[21:17:03] Orchestrator: Saved JSON report to outputs\json\ahmedabad_residential.json
[21:17:03] Orchestrator: Saved geometry to outputs\geometry\ahmedabad_residential.json
[21:17:03] Orchestrator: Run complete.
[23:18:17] Feedback received for case test-down-manual: down
[23:31:17] Feedback received for case test-case-001: up
[23:59:24] Orchestrator: Starting run for case: data/inputs/mumbai_case.json
[23:59:24] InputAgent: Loading case from data\inputs\mumbai_case.json
[23:59:24] InputAgent: Successfully loaded case with keys: ['case_id', 'location', 'road_width', 'plot_size']
[23:59:24] Would download: https://example.com/regulations.pdf
[23:59:24] Parsing (stub): rules_kb/sample_rules.json
[23:59:24] Classification Agent: Starting to select applicable rules.
[23:59:24] Classification Agent: REJECTED rule MUM-DCPR-2034-SETBACK-FRONT
[23:59:24] Classification Agent: REJECTED rule MUM-DCPR-2034-COVERAGE
[23:59:24] Classification Agent: REJECTED rule MUM-DCPR-2034-FAR
[23:59:24] Classification Agent: REJECTED rule MUM-MCGM-HEIGHT
[23:59:24] Classification Agent: REJECTED rule MUM-MHADA-REDEVELOPMENT
[23:59:24] Classification Agent: REJECTED rule AHM-DCR-SETBACK
[23:59:24] Classification Agent: REJECTED rule AHM-DCR-COVERAGE
[23:59:24] Classification Agent: REJECTED rule AHM-DCR-FAR
[23:59:24] Classification Agent: Found 0 applicable rules.
[23:59:24] Classification Agent: Decision details - REJECTED rule MUM-DCPR-2034-SETBACK-FRONT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-COVERAGE: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-DCPR-2034-FAR: Condition mismatch: city=None, expected Mumbai, Condition mismatch: plot_type=None, expected residential, REJECTED rule MUM-MCGM-HEIGHT: Condition mismatch: city=None, expected Mumbai, REJECTED rule MUM-MHADA-REDEVELOPMENT: Condition mismatch: city=None, expected Mumbai, Condition mismatch: project_type=None, expected slum_redevelopment, REJECTED rule AHM-DCR-SETBACK: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-COVERAGE: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential, REJECTED rule AHM-DCR-FAR: Condition mismatch: city=None, expected Ahmedabad, Condition mismatch: plot_type=None, expected residential
[23:59:24] Calculation Agent: Starting computations.
[23:59:24] Calculation Agent: Processing case with plot size: 500 sqm
[23:59:24] Calculation Agent: Location is urban
[23:59:24] Calculation Agent: No specific setback rule found, using default: 1.5
[23:59:24] Calculation Agent: No specific coverage rule found, using default: 0.55
[23:59:24] Calculation Agent: No specific FAR rule found, using default: 1.5
[23:59:24] Calculation Agent: No additional FAR rule found
[23:59:24] Calculation Agent: Max footprint: 275.0 sqm, Total floor area: 750.0 sqm
[23:59:24] Calculation Agent: Finished. Total floor area: 750.0 sqm.
[23:59:24] RL Agent: Starting RL decision process.
[23:59:24] RL Agent: Expected rule path: []
[23:59:24] RL Agent: Candidate 1 (expected): []
[23:59:24] RL Agent: Candidate 2 (reversed): []
[23:59:24] RL Agent: Initializing RL environment and training...
[23:59:24] RL Environment: Initialized with 2 candidate paths
[23:59:24] RL Environment: Candidate 0: []
[23:59:24] RL Environment: Candidate 1: []
[23:59:24] Training RL (random policy) for 10 episodes...
[23:59:24] RL Training Summary: {'status': 'training_started', 'episodes': 10}
[23:59:24] RL Environment: Reset
[23:59:24] RL Environment: Feedback reward for case mumbai-001: 0
[23:59:24] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[23:59:24] RL Training - Episode 1: {'episode': 1, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:59:24] EP 1: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[23:59:24] RL Environment: Reset
[23:59:24] RL Environment: Feedback reward for case mumbai-001: 0
[23:59:24] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[23:59:24] RL Training - Episode 2: {'episode': 2, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:59:24] EP 2: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[23:59:24] RL Environment: Reset
[23:59:24] RL Environment: Feedback reward for case mumbai-001: 0
[23:59:24] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[23:59:24] RL Training - Episode 3: {'episode': 3, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:59:24] EP 3: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[23:59:24] RL Environment: Reset
[23:59:24] RL Environment: Feedback reward for case mumbai-001: 0
[23:59:24] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[23:59:24] RL Training - Episode 4: {'episode': 4, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:59:24] EP 4: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[23:59:24] RL Environment: Reset
[23:59:24] RL Environment: Feedback reward for case mumbai-001: 0
[23:59:24] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[23:59:24] RL Training - Episode 5: {'episode': 5, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:59:24] EP 5: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[23:59:24] RL Environment: Reset
[23:59:24] RL Environment: Feedback reward for case mumbai-001: 0
[23:59:24] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[23:59:24] RL Training - Episode 6: {'episode': 6, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:59:24] EP 6: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[23:59:24] RL Environment: Reset
[23:59:24] RL Environment: Feedback reward for case mumbai-001: 0
[23:59:24] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[23:59:24] RL Training - Episode 7: {'episode': 7, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:59:24] EP 7: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[23:59:24] RL Environment: Reset
[23:59:24] RL Environment: Feedback reward for case mumbai-001: 0
[23:59:24] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[23:59:24] RL Training - Episode 8: {'episode': 8, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:59:24] EP 8: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[23:59:24] RL Environment: Reset
[23:59:24] RL Environment: Feedback reward for case mumbai-001: 0
[23:59:24] RL Environment: Action 1 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[23:59:24] RL Training - Episode 9: {'episode': 9, 'action': 1, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:59:24] EP 9: action=1, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[23:59:24] RL Environment: Reset
[23:59:24] RL Environment: Feedback reward for case mumbai-001: 0
[23:59:24] RL Environment: Action 0 -> Chosen: [], Expected: [], Base Reward: 1, Feedback Reward: 0, Total Reward: 1
[23:59:24] RL Training - Episode 10: {'episode': 10, 'action': 0, 'base_reward': 1, 'feedback_reward': 0, 'total_reward': 1, 'chosen_path': [], 'expected_path': [], 'running_avg_reward': 1.0, 'success_rate': 1.0}
[23:59:24] EP 10: action=0, base_reward=1, feedback_reward=0, total_reward=1, chosen=[] expected=[]
[23:59:24] Avg reward: 1.00, Success rate: 100.00%
[23:59:24] RL Training Summary: {'status': 'training_completed', 'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10, 'total_reward': 10}
[23:59:24] RL Agent: Training complete. Metrics: {'avg_reward': 1.0, 'success_rate': 1.0, 'episodes': 10}
[23:59:24] RL Agent: Process complete.
[23:59:24] Orchestrator: Saved JSON report to outputs\json\mumbai_output.json
[23:59:24] Orchestrator: Saved geometry to outputs\geometry\mumbai_model.stl
[23:59:24] Orchestrator: Run complete.
[00:23:47] Feedback received for case test-case-001: up
[01:23:06] Feedback received for case test-case-001: up
[01:45:27] Feedback received for case ui-test-case: up
[01:45:52] Feedback received for case ui-actual-test: down
[01:51:22] Feedback received for case demo-20251006-015120: up
[01:52:09] Feedback received for case demo-20251006-015207: up
[01:54:18] Feedback received for case demo-up-20251006-015416: up
[01:54:21] Feedback received for case demo-down-20251006-015419: down
